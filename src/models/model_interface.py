# src/models/model_interface.py
import asyncio
import json
import time
from typing import List, Dict, Any, Optional, Tuple
from abc import ABC, abstractmethod

import openai
from anthropic import Anthropic
import google.generativeai as genai

from src.models.data_models import ModelResponse
from src.utils.config_loader import ConfigLoader
from src.utils.logger import get_logger, log_model_call, log_error

class AIModelInterface(ABC):
    """AI ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ ì¶”ìƒ í´ë˜ìŠ¤"""
    
    def __init__(self, api_key: str, model_name: str):
        self.api_key = api_key
        self.model_name = model_name
        self.max_retries = 3
        self.timeout = 90  # 90ì´ˆë¡œ ì¦ê°€ (ë°©ë³„ ì²˜ë¦¬ ì‹œ ë” ë§ì€ ì‹œê°„ í•„ìš”)
        self.logger = get_logger(self.__class__.__name__)
    
    @abstractmethod
    async def call_model(self, prompt: str, json_data: Dict[str, Any]) -> ModelResponse:
        """ëª¨ë¸ í˜¸ì¶œ ì¶”ìƒ ë©”ì„œë“œ"""
        pass
    
    def _prepare_prompt(self, base_prompt: str, json_data: Dict[str, Any]) -> str:
        """í”„ë¡¬í”„íŠ¸ ì¤€ë¹„"""
        json_str = json.dumps(json_data, indent=2, ensure_ascii=False)
        return f"{base_prompt}\n\n[JSON DATA]\n{json_str}"
    
    def _extract_response_data(self, raw_response: str) -> Dict[str, Any]:
        """ì‘ë‹µì—ì„œ êµ¬ì¡°í™”ëœ ë°ì´í„° ì¶”ì¶œ - Phase 1/2 ì§€ì›"""
        try:
            # ì‘ë‹µì´ ë¹„ì–´ìˆê±°ë‚˜ ì—ëŸ¬ì¸ ê²½ìš° ë¨¼ì € ì²´í¬
            if not raw_response or len(raw_response.strip()) < 10:
                self.logger.warning(f"ì‘ë‹µì´ ë„ˆë¬´ ì§§ê±°ë‚˜ ë¹„ì–´ìˆìŒ: {raw_response[:100]}")
                return {'work_items': [], 'rooms': [], 'parse_error': 'Empty response'}
            
            # 1. JSON ì‘ë‹µ ì²˜ë¦¬ ì‹œë„
            parsed_json = self._try_parse_json(raw_response)
            if parsed_json:
                result = self._process_structured_response(parsed_json)
                self.logger.info(f"JSON íŒŒì‹± ì„±ê³µ: {len(result.get('rooms', []))} ë°©, {len(result.get('work_items', []))} ì‘ì—…")
                return result
            
            # 2. í…ìŠ¤íŠ¸ ì‘ë‹µì—ì„œ êµ¬ì¡°í™”ëœ ë°ì´í„° ì¶”ì¶œ
            result = self._parse_text_response(raw_response)
            self.logger.info(f"í…ìŠ¤íŠ¸ íŒŒì‹± ì™„ë£Œ: {len(result.get('rooms', []))} ë°©, {len(result.get('work_items', []))} ì‘ì—…")
            return result
            
        except Exception as e:
            self.logger.error(f"ì‘ë‹µ íŒŒì‹± ì˜¤ë¥˜: {e}")
            self.logger.debug(f"ì›ë³¸ ì‘ë‹µ (ì²˜ìŒ 500ì): {raw_response[:500]}")
            return {
                'work_items': [],
                'rooms': [],
                'parse_error': str(e),
                'raw_text': raw_response
            }
    
    def _try_parse_json(self, raw_response: str) -> Optional[Dict[str, Any]]:
        """JSON íŒŒì‹± ì‹œë„ (ì—¬ëŸ¬ í˜•íƒœì˜ JSON ì§€ì›)"""
        response = raw_response.strip()
        
        # ì§ì ‘ JSONì¸ ê²½ìš°
        if response.startswith('{') or response.startswith('['):
            try:
                return json.loads(response)
            except json.JSONDecodeError:
                pass
        
        # ì½”ë“œ ë¸”ë¡ ì•ˆì— JSONì´ ìˆëŠ” ê²½ìš°
        import re
        json_patterns = [
            r'```json\s*(\{.*?\})\s*```',
            r'```\s*(\{.*?\})\s*```',
            r'`(\{.*?\})`',
            r'(\{[\s\S]*\})'  # ë§ˆì§€ë§‰ ì‹œë„: ê°€ì¥ í° JSON ê°ì²´
        ]
        
        for pattern in json_patterns:
            matches = re.findall(pattern, response, re.DOTALL)
            for match in matches:
                try:
                    return json.loads(match.strip())
                except json.JSONDecodeError:
                    continue
        
        return None
    
    def _process_structured_response(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """êµ¬ì¡°í™”ëœ JSON ì‘ë‹µ ì²˜ë¦¬"""
        result = {
            'work_items': [],
            'rooms': [],
            'phase_info': {}
        }
        
        # Phase ì •ë³´ ì¶”ì¶œ
        if 'phase' in data:
            result['phase_info'] = {
                'phase': data.get('phase'),
                'phase_name': data.get('phase_name', ''),
                'model_used': data.get('model_used', ''),
                'timestamp': data.get('timestamp', '')
            }
        
        # GPT-4 structured output í˜•ì‹ (phase í¬í•¨)
        if 'phase' in data and data.get('phase') in ['phase1_work_scope', '1', 'phase1']:
            # ì´ë¯¸ ì˜¬ë°”ë¥¸ êµ¬ì¡°í™”ëœ ì‘ë‹µ
            for room in data.get('rooms', []):
                processed_room = {
                    'name': room.get('room_name', 'ì•Œ ìˆ˜ ì—†ëŠ” ë°©'),
                    'tasks': []
                }
                
                # tasks ë°°ì—´ ì§ì ‘ ì²˜ë¦¬
                for task in room.get('tasks', []):
                    normalized_task = {
                        'task_name': task.get('task_name', ''),
                        'description': task.get('notes', ''),
                        'necessity': 'required',
                        'quantity': task.get('quantity', 0.0),
                        'unit': task.get('unit', ''),
                        'room_name': room.get('room_name', ''),
                        'reasoning': task.get('reasoning', task.get('notes', '')),
                        'task_type': task.get('task_type', ''),
                        'material_category': task.get('material_category', '')
                    }
                    processed_room['tasks'].append(normalized_task)
                    result['work_items'].append(normalized_task)
                
                result['rooms'].append(processed_room)
        
        # í”„ë¡œì íŠ¸ ë°ì´í„° êµ¬ì¡°ì¸ ê²½ìš° (Phase 0/1 í˜•íƒœ)
        elif 'data' in data and isinstance(data['data'], list):
            project_data = data['data']
            if len(project_data) > 1:
                # floors ë°ì´í„°ì—ì„œ rooms ì¶”ì¶œ
                for floor_data in project_data[1:]:
                    if 'rooms' in floor_data:
                        for room in floor_data['rooms']:
                            processed_room = self._process_room_data(room)
                            result['rooms'].append(processed_room)
                            result['work_items'].extend(processed_room.get('tasks', []))
        
        # ì§ì ‘ rooms ë°°ì—´ì¸ ê²½ìš°
        elif 'rooms' in data:
            for room in data['rooms']:
                processed_room = self._process_room_data(room)
                result['rooms'].append(processed_room)
                result['work_items'].extend(processed_room.get('tasks', []))
        
        # work_items ì§ì ‘ í¬í•¨ì¸ ê²½ìš°
        elif 'work_items' in data:
            result['work_items'] = data['work_items']
            # work_itemsì—ì„œ rooms ì¬êµ¬ì„±
            result['rooms'] = self._group_work_items_by_room(data['work_items'])
        
        # ë‹¨ì¼ ì‘ì—… ëª©ë¡ì¸ ê²½ìš° (ë¦¬ìŠ¤íŠ¸ í˜•íƒœ)
        elif isinstance(data, list):
            for item in data:
                if isinstance(item, dict) and ('task_name' in item or 'name' in item):
                    result['work_items'].append(self._normalize_work_item(item))
            result['rooms'] = self._group_work_items_by_room(result['work_items'])
        
        return result
    
    def _process_room_data(self, room_data: Dict[str, Any]) -> Dict[str, Any]:
        """ë°© ë°ì´í„° ì²˜ë¦¬ ë° ì‘ì—… í•­ëª© ì¶”ì¶œ"""
        room_name = room_data.get('room_name') or room_data.get('name', 'ì•Œ ìˆ˜ ì—†ëŠ” ë°©')
        
        processed_room = {
            'name': room_name,
            'material': room_data.get('material', {}),
            'work_scope': room_data.get('work_scope', {}),
            'measurements': room_data.get('measurements', {}),
            'demo_scope': room_data.get('demo_scope(already demo\'d)', {}),
            'additional_notes': room_data.get('additional_notes', {}),
            'tasks': []
        }
        
        # ë¨¼ì € tasks í•„ë“œê°€ ì§ì ‘ ìˆëŠ”ì§€ í™•ì¸ (GPT-4 structured output)
        if 'tasks' in room_data and isinstance(room_data['tasks'], list):
            for task in room_data['tasks']:
                normalized_task = self._normalize_work_item(task)
                normalized_task['room'] = room_name
                processed_room['tasks'].append(normalized_task)
        
        # tasksê°€ ì—†ìœ¼ë©´ work_scopeì—ì„œ ìƒì„± ì‹œë„
        elif room_data.get('work_scope'):
            tasks = self._extract_tasks_from_room(room_data, room_name)
            processed_room['tasks'] = tasks
        
        # Phase 2: ìˆ˜ëŸ‰ ì •ë³´ê°€ í¬í•¨ëœ ê²½ìš° ì²˜ë¦¬
        if 'quantity_estimates' in room_data:
            self._add_quantity_info(processed_room['tasks'], room_data['quantity_estimates'])
        
        return processed_room
    
    def _extract_tasks_from_room(self, room_data: Dict[str, Any], room_name: str) -> List[Dict[str, Any]]:
        """ë°© ë°ì´í„°ì—ì„œ ì‘ì—… í•­ëª© ì¶”ì¶œ (Remove & Replace ë¡œì§ ì ìš©)"""
        tasks = []
        work_scope = room_data.get('work_scope', {})
        measurements = room_data.get('measurements', {})
        demo_scope = room_data.get('demo_scope(already demo\'d)', {})
        
        # ê° ì‘ì—… ì˜ì—­ë³„ë¡œ ì²˜ë¦¬
        scope_mappings = {
            'Flooring': 'floor_area_sqft',
            'Wall': 'wall_area_sqft', 
            'Ceiling': 'ceiling_area_sqft',
            'Baseboard': 'floor_perimeter_lf',
            'Quarter Round': 'floor_perimeter_lf'
        }
        
        for scope_type, area_key in scope_mappings.items():
            scope_value = work_scope.get(scope_type, '').strip()
            if not scope_value or scope_value.lower() in ['', 'n/a', 'none']:
                continue
            
            # ì¸¡ì •ê°’ ê°€ì ¸ì˜¤ê¸°
            area_value = measurements.get(area_key, 0.0)
            unit = 'sqft' if 'area' in area_key else 'lf'
            
            # Remove & Replace ë¡œì§ ì ìš©
            if scope_value == "Remove & Replace":
                # ì² ê±°ëŸ‰ í™•ì¸
                demo_amount = self._get_demo_amount(demo_scope, scope_type)
                remaining_area = max(0, area_value - demo_amount)
                
                # ì œê±° ì‘ì—… (ë‚¨ì€ ë¶€ë¶„ë§Œ)
                if remaining_area > 0:
                    tasks.append({
                        'task_name': f'Remove existing {scope_type.lower()}',
                        'description': f'Remove existing {scope_type.lower()} material',
                        'necessity': 'required',
                        'quantity': remaining_area,
                        'unit': unit,
                        'room_name': room_name,
                        'reasoning': f'Remove & Replace scope - Demo already done: {demo_amount} {unit}'
                    })
                
                # ì„¤ì¹˜ ì‘ì—… (ì „ì²´ ë©´ì )
                tasks.append({
                    'task_name': f'Install new {scope_type.lower()}',
                    'description': f'Install new {scope_type.lower()} material',
                    'necessity': 'required',
                    'quantity': area_value,
                    'unit': unit,
                    'room_name': room_name,
                    'reasoning': f'Remove & Replace scope - Full area installation required'
                })
            
            elif scope_value in ["Paint", "Patch"]:
                # í˜ì¸íŠ¸/íŒ¨ì¹˜ ì‘ì—…
                tasks.append({
                    'task_name': f'{scope_value} {scope_type.lower()}',
                    'description': f'{scope_value} {scope_type.lower()} surface',
                    'necessity': 'required',
                    'quantity': area_value,
                    'unit': unit,
                    'room_name': room_name,
                    'reasoning': f'{scope_value} work specified'
                })
        
        # ì¶”ê°€ ì‘ì—… (ë³´í˜¸, ë¶„ë¦¬/ì¬ì„¤ì¹˜ ë“±)
        additional_tasks = self._extract_additional_tasks(room_data, room_name)
        tasks.extend(additional_tasks)
        
        return tasks
    
    def _get_demo_amount(self, demo_scope: Dict[str, Any], scope_type: str) -> float:
        """ì² ê±° ì™„ë£Œëœ ìˆ˜ëŸ‰ í™•ì¸"""
        demo_mappings = {
            'Wall': 'Wall Drywall(sq_ft)',
            'Ceiling': 'Ceiling Drywall(sq_ft)'
        }
        
        demo_key = demo_mappings.get(scope_type)
        if demo_key:
            return demo_scope.get(demo_key, 0.0)
        return 0.0
    
    def _extract_additional_tasks(self, room_data: Dict[str, Any], room_name: str) -> List[Dict[str, Any]]:
        """ì¶”ê°€ ì‘ì—… í•­ëª© ì¶”ì¶œ (ë³´í˜¸, ë¶„ë¦¬/ì¬ì„¤ì¹˜ ë“±)"""
        tasks = []
        additional_notes = room_data.get('additional_notes', {})
        
        # ë³´í˜¸ ì‘ì—…
        protection_items = additional_notes.get('protection', [])
        for item in protection_items:
            tasks.append({
                'task_name': f'Protection: {item}',
                'description': f'Provide {item}',
                'necessity': 'required',
                'quantity': 1,
                'unit': 'item',
                'room_name': room_name,
                'reasoning': 'Protection requirement specified'
            })
        
        # ë¶„ë¦¬/ì¬ì„¤ì¹˜ ì‘ì—…
        detach_reset_items = additional_notes.get('detach_reset', [])
        for item in detach_reset_items:
            tasks.extend([
                {
                    'task_name': f'Detach {item}',
                    'description': f'Carefully detach {item}',
                    'necessity': 'required',
                    'quantity': 1,
                    'unit': 'item',
                    'room_name': room_name,
                    'reasoning': 'Detach/reset requirement specified'
                },
                {
                    'task_name': f'Reset {item}',
                    'description': f'Reinstall {item}',
                    'necessity': 'required',
                    'quantity': 1,
                    'unit': 'item',
                    'room_name': room_name,
                    'reasoning': 'Detach/reset requirement specified'
                }
            ])
        
        return tasks
    
    def _parse_text_response(self, raw_response: str) -> Dict[str, Any]:
        """í…ìŠ¤íŠ¸ ì‘ë‹µ íŒŒì‹± (í–¥ìƒëœ ë²„ì „)"""
        lines = raw_response.split('\n')
        work_items = []
        current_room = ""
        current_context = {}
        
        # íŒ¨í„´ ì¸ì‹ì„ ìœ„í•œ ì •ê·œì‹
        import re
        room_pattern = re.compile(r'(?:room|kitchen|bathroom|bedroom|living|dining)\s*:?\s*([^,\n]*)', re.IGNORECASE)
        task_pattern = re.compile(r'^[\s]*[-*â€¢]|^[\s]*\d+[\.]|^[\s]*\w+:')
        quantity_pattern = re.compile(r'(\d+(?:\.\d+)?)\s*(sqft|sq\s*ft|lf|sf|sy)', re.IGNORECASE)
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            # ë°© ì´ë¦„ ê°ì§€
            room_match = room_pattern.search(line)
            if room_match:
                current_room = room_match.group(1).strip() or room_match.group(0).strip()
                continue
            
            # ì‘ì—… í•­ëª© ê°ì§€
            if task_pattern.match(line):
                task_text = re.sub(r'^[\s]*[-*â€¢\d\.]+\s*', '', line).strip()
                if not task_text:
                    continue
                
                # ìˆ˜ëŸ‰ ì •ë³´ ì¶”ì¶œ
                quantity = 0.0
                unit = ""
                quantity_match = quantity_pattern.search(task_text)
                if quantity_match:
                    quantity = float(quantity_match.group(1))
                    unit = quantity_match.group(2).replace(' ', '').lower()
                
                work_item = {
                    'task_name': task_text,
                    'description': task_text,
                    'necessity': 'required',
                    'quantity': quantity,
                    'unit': unit,
                    'room_name': current_room,
                    'reasoning': 'Extracted from text response'
                }
                
                work_items.append(work_item)
        
        # ë°©ë³„ë¡œ ê·¸ë£¹í™”
        rooms = self._group_work_items_by_room(work_items)
        
        return {
            'work_items': work_items,
            'rooms': rooms
        }
    
    def _group_work_items_by_room(self, work_items: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ì‘ì—… í•­ëª©ì„ ë°©ë³„ë¡œ ê·¸ë£¹í™”"""
        room_groups = {}
        
        for item in work_items:
            room_name = item.get('room_name', 'ë¯¸ë¶„ë¥˜')
            if room_name not in room_groups:
                room_groups[room_name] = {
                    'name': room_name,
                    'tasks': []
                }
            room_groups[room_name]['tasks'].append(item)
        
        return list(room_groups.values())
    
    def _normalize_work_item(self, item: Dict[str, Any]) -> Dict[str, Any]:
        """ì‘ì—… í•­ëª© ì •ê·œí™”"""
        return {
            'task_name': item.get('task_name') or item.get('name', 'ì•Œ ìˆ˜ ì—†ëŠ” ì‘ì—…'),
            'description': item.get('description', item.get('task_name', item.get('name', ''))),
            'necessity': item.get('necessity', 'required'),
            'quantity': item.get('quantity', 0.0),
            'unit': item.get('unit', ''),
            'room_name': item.get('room_name', ''),
            'reasoning': item.get('reasoning', '')
        }
    
    def _add_quantity_info(self, tasks: List[Dict[str, Any]], quantity_estimates: Dict[str, Any]):
        """Phase 2 ìˆ˜ëŸ‰ ì •ë³´ ì¶”ê°€"""
        # ìˆ˜ëŸ‰ ì •ë³´ê°€ ìˆëŠ” ê²½ìš° ê¸°ì¡´ ì‘ì—…ì— ì¶”ê°€
        for task in tasks:
            task_name = task.get('task_name', '').lower()
            
            # ë§¤ì¹­ë˜ëŠ” ìˆ˜ëŸ‰ ì •ë³´ ì°¾ê¸°
            for qty_key, qty_value in quantity_estimates.items():
                if any(keyword in task_name for keyword in qty_key.lower().split()):
                    if isinstance(qty_value, dict):
                        task['quantity'] = qty_value.get('quantity', task['quantity'])
                        task['unit'] = qty_value.get('unit', task['unit'])
                        task['cost_estimate'] = qty_value.get('cost', 0.0)
                    elif isinstance(qty_value, (int, float)):
                        task['quantity'] = qty_value
    
    def _log_model_response(self, model_name: str, raw_response: str, response_time: float):
        """AI ëª¨ë¸ ì‘ë‹µì„ íŒŒì¼ê³¼ ì½˜ì†”ì— ë¡œê¹…"""
        import os
        from datetime import datetime
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        debug_dir = "ai_responses"
        os.makedirs(debug_dir, exist_ok=True)
        
        # íƒ€ì„ìŠ¤íƒ¬í”„ ìƒì„±
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # íŒŒì¼ëª… ìƒì„±
        clean_name = model_name.lower().replace('-', '_').replace(' ', '_')
        debug_file = f"{debug_dir}/{clean_name}_response_{timestamp}.txt"
        
        # íŒŒì¼ì— ì €ì¥
        with open(debug_file, 'w', encoding='utf-8') as f:
            f.write(f"{model_name} Response\n")
            f.write(f"ì²˜ë¦¬ ì‹œê°„: {response_time:.2f}ì´ˆ\n")
            f.write(f"ì‘ë‹µ í¬ê¸°: {len(raw_response)} characters\n")
            f.write("="*80 + "\n")
            f.write(raw_response)
            f.write("\n" + "="*80 + "\n")
            
            # JSON íŒŒì‹± ì‹œë„
            try:
                import json
                parsed = self._try_parse_json(raw_response)
                if parsed:
                    f.write("\n[Parsed JSON Structure]\n")
                    f.write(json.dumps(parsed, indent=2, ensure_ascii=False)[:5000])  # ì²˜ìŒ 5000ìë§Œ
            except:
                pass
        
        # ì½˜ì†”ì— ìš”ì•½ ì¶œë ¥
        print(f"\n" + "="*80)
        print(f"ğŸ“ {model_name} AI ì‘ë‹µ ìˆ˜ì‹ ")
        print(f"â±ï¸  ì²˜ë¦¬ ì‹œê°„: {response_time:.2f}ì´ˆ")
        print(f"ğŸ“Š ì‘ë‹µ í¬ê¸°: {len(raw_response)} characters")
        print(f"ğŸ’¾ ì €ì¥ ìœ„ì¹˜: {debug_file}")
        
        # ì‘ë‹µ ë¯¸ë¦¬ë³´ê¸° (ì²˜ìŒ 500ì)
        preview = raw_response[:500]
        if len(raw_response) > 500:
            preview += "... (truncated)"
        print(f"\n[ì‘ë‹µ ë¯¸ë¦¬ë³´ê¸°]\n{preview}")
        print("="*80 + "\n")
        
        # ì‘ì—… ê°œìˆ˜ í™•ì¸
        try:
            data = self._extract_response_data(raw_response)
            work_items = data.get('work_items', [])
            rooms = data.get('rooms', [])
            
            total_tasks = len(work_items)
            if total_tasks == 0 and rooms:
                # roomsì—ì„œ tasks ê³„ì‚°
                for room in rooms:
                    total_tasks += len(room.get('tasks', []))
            
            if total_tasks == 0:
                print(f"âš ï¸  ê²½ê³ : {model_name}ì—ì„œ ì‘ì—…ì´ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")
                self.logger.warning(f"{model_name} generated 0 tasks")
            else:
                print(f"âœ… {model_name}: {total_tasks}ê°œ ì‘ì—… ìƒì„±")
                self.logger.info(f"{model_name} generated {total_tasks} tasks")
        except Exception as e:
            print(f"âš ï¸  ì‘ì—… ê°œìˆ˜ íŒŒì‹± ì‹¤íŒ¨: {e}")
            self.logger.error(f"Failed to parse task count: {e}")

class GPT4Interface(AIModelInterface):
    """GPT-4 ì¸í„°í˜ì´ìŠ¤"""
    
    def __init__(self, api_key: str, model_name: str = None):
        self.actual_model_name = model_name or "gpt-4o-mini"
        super().__init__(api_key, self.actual_model_name)
        self.client = openai.AsyncOpenAI(api_key=api_key)
        self.logger = get_logger('gpt4_interface')
        self._last_api_response = None  # Store raw API response for token tracking
    
    async def call_model(self, prompt: str, json_data: Dict[str, Any]) -> ModelResponse:
        """GPT-4 ëª¨ë¸ í˜¸ì¶œ with Structured Outputs"""
        start_time = time.time()
        
        try:
            full_prompt = self._prepare_prompt(prompt, json_data)
            
            self.logger.info("GPT-4 API í˜¸ì¶œ ì‹œì‘ (Structured Output Mode)")
            self.logger.debug(f"í”„ë¡¬í”„íŠ¸ í¬ê¸°: {len(full_prompt)} characters")
            log_model_call(self.actual_model_name, len(full_prompt))
            
            # Structured Output JSON ìŠ¤í‚¤ë§ˆ ì •ì˜ - Phase 1: Work Scope Only (NO COSTS)
            response_format = {
                "type": "json_schema",
                "json_schema": {
                    "name": "phase1_work_scope",
                    "strict": True,
                    "schema": {
                        "type": "object",
                        "additionalProperties": False,
                        "properties": {
                            "phase": {"type": "string"},
                            "processing_timestamp": {"type": "string"},
                            "rooms": {
                                "type": "array",
                                "items": {
                                    "type": "object",
                                    "additionalProperties": False,
                                    "properties": {
                                        "room_name": {"type": "string"},
                                        "room_id": {"type": "string"},
                                        "tasks": {
                                            "type": "array",
                                            "items": {
                                                "type": "object",
                                                "additionalProperties": False,
                                                "properties": {
                                                    "task_id": {"type": "string"},
                                                    "task_name": {"type": "string"},
                                                    "task_type": {
                                                        "type": "string",
                                                        "enum": ["removal", "installation", "protection", "detach", "reset", "preparation", "cleaning", "disposal", "finishing", "repair", "other"]
                                                    },
                                                    "material_category": {
                                                        "type": "string",
                                                        "enum": ["flooring", "wall", "ceiling", "baseboard", "other"]
                                                    },
                                                    "quantity": {"type": "number"},
                                                    "unit": {
                                                        "type": "string",
                                                        "enum": ["sqft", "lf", "sy", "item", "hour", "each"]
                                                    },
                                                    "notes": {"type": "string"},
                                                    "high_ceiling_premium_applied": {"type": "boolean"},
                                                    "demo_already_completed": {"type": "number"}
                                                },
                                                "required": ["task_id", "task_name", "task_type", "material_category", "quantity", "unit", "notes", "high_ceiling_premium_applied", "demo_already_completed"]
                                            }
                                        },
                                        "room_totals": {
                                            "type": "object",
                                            "additionalProperties": False,
                                            "properties": {
                                                "total_tasks": {"type": "number"},
                                                "total_removal_tasks": {"type": "number"},
                                                "total_installation_tasks": {"type": "number"}
                                            },
                                            "required": ["total_tasks", "total_removal_tasks", "total_installation_tasks"]
                                        }
                                    },
                                    "required": ["room_name", "room_id", "tasks", "room_totals"]
                                }
                            },
                            "summary": {
                                "type": "object",
                                "additionalProperties": False,
                                "properties": {
                                    "total_rooms": {"type": "number"},
                                    "total_tasks": {"type": "number"},
                                    "has_high_ceiling_areas": {"type": "boolean"},
                                    "validation_status": {
                                        "type": "object",
                                        "additionalProperties": False,
                                        "properties": {
                                            "remove_replace_logic_applied": {"type": "boolean"},
                                            "measurements_used": {"type": "boolean"},
                                            "special_tasks_included": {"type": "boolean"}
                                        },
                                        "required": ["remove_replace_logic_applied", "measurements_used", "special_tasks_included"]
                                    }
                                },
                                "required": ["total_rooms", "total_tasks", "has_high_ceiling_areas", "validation_status"]
                            }
                        },
                        "required": ["phase", "processing_timestamp", "rooms", "summary"]
                    }
                }
            }
            
            # Check if model supports structured outputs
            if self.actual_model_name in ["gpt-4o", "gpt-4o-mini", "gpt-4-turbo-preview"]:
                response = await self.client.chat.completions.create(
                    model=self.actual_model_name,
                    messages=[
                        {
                            "role": "system", 
                            "content": """You are a Senior Reconstruction Estimating Specialist. Your task is to:
1. Analyze room data and generate COMPREHENSIVE task lists (10-20 tasks per room minimum)
2. Apply Remove & Replace logic correctly (removal for remaining + installation for full area)
3. Include ALL necessary tasks: removal, disposal, preparation, installation, finishing, protection, cleanup
4. Use the EXACT JSON structure specified in the schema
5. Ensure every room has substantial tasks even for simple work scopes
6. NEVER return empty task arrays - every room MUST have multiple detailed tasks
7. ALWAYS fill the 'notes' field with detailed reasoning explaining WHY this task is needed
8. Include specific justification in notes like: "Required due to Remove & Replace scope", "Necessary for proper surface preparation", "Essential for safety compliance", etc."""
                        },
                        {"role": "user", "content": full_prompt}
                    ],
                    response_format=response_format,
                    max_tokens=4000,
                    temperature=0.1,
                    timeout=self.timeout
                )
                
                # Store raw response for token tracking
                self._last_api_response = response
            else:
                # Fallback for older models
                response = await self.client.chat.completions.create(
                    model=self.actual_model_name,
                    messages=[
                        {
                            "role": "system", 
                            "content": "You are a Senior Reconstruction Estimating Specialist. Analyze the data carefully and provide detailed work scope estimates. Return ONLY valid JSON matching the specified schema."
                        },
                        {"role": "user", "content": full_prompt}
                    ],
                    max_tokens=3000,
                    temperature=0.1,
                    timeout=self.timeout
                )
                
                # Store raw response for token tracking
                self._last_api_response = response
            
            raw_response = response.choices[0].message.content
            response_time = time.time() - start_time
            
            self.logger.info(f"GPT-4 ì‘ë‹µ ìˆ˜ì‹  (ì†Œìš”ì‹œê°„: {response_time:.2f}ì´ˆ)")
            self.logger.debug(f"ì‘ë‹µ í¬ê¸°: {len(raw_response)} characters")
            
            # AI ì‘ë‹µ íŒŒì¼ê³¼ ì½˜ì†”ì— ë¡œê¹…
            self._log_model_response('GPT-4', raw_response, response_time)
            
            processed_data = self._extract_response_data(raw_response)
            
            return ModelResponse(
                model_name=self.model_name,
                room_estimates=processed_data.get('rooms', []),
                processing_time=time.time() - start_time,
                total_work_items=len(processed_data.get('work_items', [])),
                raw_response=raw_response,
                confidence_self_assessment=0.85  # GPT-4 ê¸°ë³¸ ì‹ ë¢°ë„
            )
            
        except Exception as e:
            self.logger.error(f"GPT-4 í˜¸ì¶œ ì˜¤ë¥˜: {e}")
            log_error('gpt4_interface', e, {'prompt_length': len(full_prompt) if 'full_prompt' in locals() else 0})
            
            # íƒ€ì„ì•„ì›ƒì´ë‚˜ ì˜¤ë¥˜ ì‹œ None ë°˜í™˜ ë˜ëŠ” ëª…ì‹œì  ì—ëŸ¬ ì‘ë‹µ
            # ModelResponse ë°˜í™˜ ì‹œ ì‹¤ì œ ë°ì´í„°ê°€ ì—†ìŒì„ ëª…í™•íˆ í‘œì‹œ
            return ModelResponse(
                model_name=self.model_name,
                room_estimates=[],
                processing_time=time.time() - start_time,
                total_work_items=0,
                raw_response=f"Error: {str(e)}",
                confidence_self_assessment=0.0
            )

class ClaudeInterface(AIModelInterface):
    """Claude ì¸í„°í˜ì´ìŠ¤"""
    
    def __init__(self, api_key: str, model_name: str = None):
        self.actual_model_name = model_name or "claude-3-5-sonnet-20241022"
        super().__init__(api_key, self.actual_model_name)
        self.client = Anthropic(api_key=api_key)
        self.logger = get_logger('claude_interface')
        self._last_api_response = None  # Store raw API response for token tracking
    
    async def call_model(self, prompt: str, json_data: Dict[str, Any]) -> ModelResponse:
        """Claude ëª¨ë¸ í˜¸ì¶œ"""
        start_time = time.time()
        
        try:
            full_prompt = self._prepare_prompt(prompt, json_data)
            
            self.logger.info("Claude API í˜¸ì¶œ ì‹œì‘")
            self.logger.debug(f"í”„ë¡¬í”„íŠ¸ í¬ê¸°: {len(full_prompt)} characters")
            log_model_call(self.actual_model_name, len(full_prompt))
            
            # asyncio.to_threadë¥¼ ì‚¬ìš©í•´ì„œ ë™ê¸° APIë¥¼ ë¹„ë™ê¸°ë¡œ ì‹¤í–‰
            response = await asyncio.to_thread(
                self.client.messages.create,
                model=self.actual_model_name,
                max_tokens=3000,
                temperature=0.1,
                messages=[{"role": "user", "content": full_prompt}]
            )
            
            # Store raw response for token tracking
            self._last_api_response = response
            
            raw_response = response.content[0].text
            response_time = time.time() - start_time
            
            self.logger.info(f"Claude ì‘ë‹µ ìˆ˜ì‹  (ì†Œìš”ì‹œê°„: {response_time:.2f}ì´ˆ)")
            self.logger.debug(f"ì‘ë‹µ í¬ê¸°: {len(raw_response)} characters")
            
            # AI ì‘ë‹µ íŒŒì¼ê³¼ ì½˜ì†”ì— ë¡œê¹…
            self._log_model_response('Claude', raw_response, response_time)
            
            processed_data = self._extract_response_data(raw_response)
            
            return ModelResponse(
                model_name=self.model_name,
                room_estimates=processed_data.get('rooms', []),
                processing_time=time.time() - start_time,
                total_work_items=len(processed_data.get('work_items', [])),
                raw_response=raw_response,
                confidence_self_assessment=0.88  # Claude ê¸°ë³¸ ì‹ ë¢°ë„ (ë³´ìˆ˜ì )
            )
            
        except Exception as e:
            self.logger.error(f"Claude í˜¸ì¶œ ì˜¤ë¥˜: {e}")
            log_error('claude_interface', e, {'prompt_length': len(full_prompt) if 'full_prompt' in locals() else 0})
            return ModelResponse(
                model_name=self.model_name,
                room_estimates=[],
                processing_time=time.time() - start_time,
                total_work_items=0,
                raw_response=f"Error: {str(e)}",
                confidence_self_assessment=0.0
            )

class GeminiInterface(AIModelInterface):
    """Gemini ì¸í„°í˜ì´ìŠ¤"""
    
    def __init__(self, api_key: str, model_name: str = None):
        self.actual_model_name = model_name or "gemini-1.5-flash"
        super().__init__(api_key, self.actual_model_name)
        genai.configure(api_key=api_key)
        self.model = genai.GenerativeModel(self.actual_model_name)
        self.logger = get_logger('gemini_interface')
        self._last_api_response = None  # Store raw API response for token tracking
    
    async def call_model(self, prompt: str, json_data: Dict[str, Any]) -> ModelResponse:
        """Gemini ëª¨ë¸ í˜¸ì¶œ"""
        start_time = time.time()
        
        try:
            full_prompt = self._prepare_prompt(prompt, json_data)
            
            self.logger.info("Gemini API í˜¸ì¶œ ì‹œì‘")
            self.logger.debug(f"í”„ë¡¬í”„íŠ¸ í¬ê¸°: {len(full_prompt)} characters")
            log_model_call(self.actual_model_name, len(full_prompt))
            
            # asyncio.to_threadë¥¼ ì‚¬ìš©í•´ì„œ ë™ê¸° APIë¥¼ ë¹„ë™ê¸°ë¡œ ì‹¤í–‰
            response = await asyncio.to_thread(
                self.model.generate_content,
                full_prompt,
                generation_config=genai.types.GenerationConfig(
                    max_output_tokens=3000,
                    temperature=0.1
                )
            )
            
            # Store raw response for token tracking
            self._last_api_response = response
            
            raw_response = response.text
            response_time = time.time() - start_time
            
            self.logger.info(f"Gemini ì‘ë‹µ ìˆ˜ì‹  (ì†Œìš”ì‹œê°„: {response_time:.2f}ì´ˆ)")
            self.logger.debug(f"ì‘ë‹µ í¬ê¸°: {len(raw_response)} characters")
            
            # AI ì‘ë‹µ íŒŒì¼ê³¼ ì½˜ì†”ì— ë¡œê¹…
            self._log_model_response('Gemini', raw_response, response_time)
            
            processed_data = self._extract_response_data(raw_response)
            
            return ModelResponse(
                model_name=self.model_name,
                room_estimates=processed_data.get('rooms', []),
                processing_time=time.time() - start_time,
                total_work_items=len(processed_data.get('work_items', [])),
                raw_response=raw_response,
                confidence_self_assessment=0.80  # Gemini ê¸°ë³¸ ì‹ ë¢°ë„
            )
            
        except Exception as e:
            self.logger.error(f"Gemini í˜¸ì¶œ ì˜¤ë¥˜: {e}")
            log_error('gemini_interface', e, {'prompt_length': len(full_prompt) if 'full_prompt' in locals() else 0})
            return ModelResponse(
                model_name=self.model_name,
                room_estimates=[],
                processing_time=time.time() - start_time,
                total_work_items=0,
                raw_response=f"Error: {str(e)}",
                confidence_self_assessment=0.0
            )

class ModelOrchestrator:
    """Enhanced model orchestrator with integrated response validation and token tracking"""
    
    def __init__(self, enable_validation: bool = True, enable_tracking: bool = True):
        self.config_loader = ConfigLoader()
        self.api_keys = self.config_loader.get_api_keys()
        self.model_names = self.config_loader.get_model_names()
        self.logger = get_logger('model_orchestrator')
        
        # Token tracking setup
        self.enable_tracking = enable_tracking
        self.token_tracker = None
        if enable_tracking:
            try:
                from src.tracking.token_tracker import TokenTracker
                self.token_tracker = TokenTracker()
                self.logger.info("Token tracking enabled")
            except ImportError as e:
                self.logger.warning(f"Token tracking unavailable: {e}")
                self.enable_tracking = False
        
        # Response validation settings
        self.enable_validation = enable_validation
        self.validation_orchestrator = None
        if enable_validation:
            try:
                from src.validators.response_validator import ValidationOrchestrator
                self.validation_orchestrator = ValidationOrchestrator()
                self.logger.info("Response validation enabled")
            except ImportError as e:
                self.logger.warning(f"Response validation unavailable: {e}")
                self.enable_validation = False
        
        # ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ ì´ˆê¸°í™”
        self.models = {}
        
        if self.api_keys['openai']:
            self.models['gpt4'] = GPT4Interface(self.api_keys['openai'], self.model_names['gpt4'])
            self.logger.info("GPT-4 ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")
        
        if self.api_keys['anthropic']:
            self.models['claude'] = ClaudeInterface(self.api_keys['anthropic'], self.model_names['claude'])
            self.logger.info("Claude ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")
        
        if self.api_keys['google']:
            self.models['gemini'] = GeminiInterface(self.api_keys['google'], self.model_names['gemini'])
            self.logger.info("Gemini ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")
        
        self.logger.info(f"ì´ {len(self.models)}ê°œ ëª¨ë¸ ì‚¬ìš© ê°€ëŠ¥, ê²€ì¦ {'í™œì„±í™”' if self.enable_validation else 'ë¹„í™œì„±í™”'}")
    
    async def run_single_model(self, model_name: str, prompt: str, json_data: Dict[str, Any]) -> Optional[ModelResponse]:
        """ë‹¨ì¼ ëª¨ë¸ ì‹¤í–‰"""
        if model_name not in self.models:
            self.logger.warning(f"ëª¨ë¸ {model_name}ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (API í‚¤ í™•ì¸)")
            return None
        
        try:
            self.logger.debug(f"ëª¨ë¸ {model_name} ì‹¤í–‰ ì‹œì‘")
            result = await self.models[model_name].call_model(prompt, json_data)
            self.logger.debug(f"ëª¨ë¸ {model_name} ì‹¤í–‰ ì™„ë£Œ")
            return result
        except Exception as e:
            self.logger.error(f"ëª¨ë¸ {model_name} ì‹¤í–‰ ì˜¤ë¥˜: {e}")
            log_error('model_orchestrator', e, {'model': model_name})
            return None
    
    async def run_parallel(self, prompt: str, json_data: Dict[str, Any], 
                          model_names: List[str] = None,
                          enable_validation: Optional[bool] = None,
                          min_quality_threshold: float = 30.0) -> List[ModelResponse]:
        """ì—¬ëŸ¬ ëª¨ë¸ ë³‘ë ¬ ì‹¤í–‰ with enhanced validation"""
        if model_names is None:
            model_names = list(self.models.keys())
        
        if enable_validation is None:
            enable_validation = self.enable_validation
        
        # ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ë§Œ í•„í„°ë§
        available_models = [name for name in model_names if name in self.models]
        
        if not available_models:
            self.logger.error("ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
            return []
        
        self.logger.info(f"ëª¨ë¸ ë³‘ë ¬ ì‹¤í–‰ ì‹œì‘: {available_models} (ê²€ì¦: {'ON' if enable_validation else 'OFF'})")
        
        # ë³‘ë ¬ ì‹¤í–‰
        tasks = [
            self.run_single_model(model_name, prompt, json_data)
            for model_name in available_models
        ]
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # ì„±ê³µí•œ ê²°ê³¼ë§Œ í•„í„°ë§ with enhanced validation
        successful_results = []
        validation_reports = []
        
        for i, result in enumerate(results):
            model_name = available_models[i]
            
            if isinstance(result, ModelResponse):
                # Enhanced validation if enabled
                if enable_validation and self.validation_orchestrator:
                    validated_response, validation_report = self._validate_response(
                        result, json_data, min_quality_threshold
                    )
                    validation_reports.append({
                        'model': model_name,
                        'report': validation_report
                    })
                    
                    if validation_report.is_valid and validation_report.quality_score >= min_quality_threshold:
                        successful_results.append(validated_response)
                        self.logger.info(
                            f"OK {model_name} - {validation_report.quality_level.value.upper()} "
                            f"({validation_report.quality_score:.1f}/100, {result.total_work_items}ê°œ ì‘ì—…)"
                        )
                    else:
                        self.logger.warning(
                            f"FAIL {model_name} - í’ˆì§ˆ ê¸°ì¤€ ë¯¸ë‹¬ "
                            f"({validation_report.quality_score:.1f}/100, "
                            f"{len(validation_report.issues)}ê°œ ì´ìŠˆ)"
                        )
                else:
                    # Fallback to basic validation (legacy behavior)
                    if result.total_work_items > 0 or (result.room_estimates and len(result.room_estimates) > 0):
                        successful_results.append(result)
                        self.logger.info(f"OK {model_name} ëª¨ë¸ ì„±ê³µ (ì‘ì—… {result.total_work_items}ê°œ)")
                    else:
                        error_msg = result.raw_response[:200] if isinstance(result.raw_response, str) else "ë¹ˆ ì‘ë‹µ"
                        self.logger.warning(f"FAIL {model_name} ëª¨ë¸ ì‘ë‹µ ë¹„ì–´ìˆìŒ: {error_msg}")
            
            elif isinstance(result, Exception):
                self.logger.error(f"FAIL {model_name} ëª¨ë¸ ì‹¤í–‰ ì¤‘ ì˜ˆì™¸: {result}")
        
        # Log validation summary
        if enable_validation and validation_reports:
            self._log_validation_summary(validation_reports)
        
        self.logger.info(f"ëª¨ë¸ ì‹¤í–‰ ì™„ë£Œ: {len(successful_results)}/{len(available_models)} ì„±ê³µ")
        return successful_results
    
    def get_available_models(self) -> List[str]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ë°˜í™˜"""
        return list(self.models.keys())
    
    def validate_api_keys(self) -> Dict[str, bool]:
        """API í‚¤ ìœ íš¨ì„± ê²€ì¦"""
        validation_results = {}
        
        for model_name, api_key in self.api_keys.items():
            is_valid = bool(api_key and api_key.strip())
            validation_results[model_name] = is_valid
            
            if is_valid:
                self.logger.debug(f"âœ“ {model_name} API í‚¤ ìœ íš¨")
            else:
                self.logger.warning(f"âœ— {model_name} API í‚¤ ì—†ìŒ")
        
        return validation_results
    
    def _validate_response(self, response: ModelResponse, 
                          original_data: Optional[Dict[str, Any]] = None,
                          min_quality_threshold: float = 30.0) -> Tuple[ModelResponse, Any]:
        """Validate and optionally fix model response"""
        try:
            # Extract response data for validation
            if hasattr(response, 'raw_response'):
                if isinstance(response.raw_response, dict):
                    response_data = response.raw_response
                else:
                    # Try to parse as JSON
                    try:
                        response_data = json.loads(response.raw_response) if isinstance(response.raw_response, str) else {}
                    except:
                        response_data = {'rooms': response.room_estimates}
            else:
                response_data = {'rooms': response.room_estimates}
                
            # Use the validation orchestrator from response_validator
            from src.validators.response_validator import validate_model_response
            validated_data, validation_report = validate_model_response(
                response_data, original_data, auto_fix=True
            )
            
            # Update response with validated data
            if 'rooms' in validated_data:
                response.room_estimates = validated_data['rooms']
                
            return response, validation_report
        except Exception as e:
            self.logger.error(f"Validation failed for {response.model_name}: {e}")
            # Return mock validation report for failed validation
            from src.validators.response_validator import ValidationReport, QualityLevel
            mock_report = ValidationReport(
                quality_score=min_quality_threshold,  # Minimum passing score
                quality_level=QualityLevel.ACCEPTABLE,
                total_issues=0,
                critical_issues=0,
                high_issues=0,
                auto_fixed=0,
                issues=[],
                processing_time=0.0,
                metadata={'validation_error': str(e)}
            )
            return response, mock_report
    
    def _log_validation_summary(self, validation_reports: List[Dict[str, Any]]) -> None:
        """Log summary of all validation reports"""
        if not validation_reports:
            return
        
        total_reports = len(validation_reports)
        valid_reports = sum(1 for r in validation_reports if r['report'].is_valid)
        
        # Calculate average quality score
        avg_quality = sum(r['report'].quality_score for r in validation_reports) / total_reports
        
        # Count quality levels
        quality_counts = {}
        total_issues = 0
        total_fixes = 0
        
        for r in validation_reports:
            report = r['report']
            quality_level = report.quality_level.value
            quality_counts[quality_level] = quality_counts.get(quality_level, 0) + 1
            total_issues += len(report.issues)
            total_fixes += report.auto_fixed
        
        self.logger.info(
            f"ğŸ” Validation Summary: {valid_reports}/{total_reports} valid, "
            f"avg quality: {avg_quality:.1f}/100"
        )
        
        if quality_counts:
            quality_summary = ", ".join([f"{level}: {count}" for level, count in quality_counts.items()])
            self.logger.info(f"ğŸ“Š Quality distribution: {quality_summary}")
        
        if total_issues > 0:
            self.logger.info(f"âš ï¸  Total issues found: {total_issues}, auto-fixes applied: {total_fixes}")
    
    def get_validation_enabled(self) -> bool:
        """Check if validation is enabled and available"""
        return self.enable_validation and self.validation_orchestrator is not None
    
    def set_validation_enabled(self, enabled: bool) -> bool:
        """Enable or disable validation (returns actual state)"""
        if enabled and self.validation_orchestrator is None:
            try:
                from src.validators.response_validator import ValidationOrchestrator
                self.validation_orchestrator = ValidationOrchestrator()
                self.enable_validation = True
                self.logger.info("Response validation enabled")
            except ImportError as e:
                self.logger.warning(f"Cannot enable validation: {e}")
                self.enable_validation = False
        else:
            self.enable_validation = enabled
            self.logger.info(f"Response validation {'enabled' if enabled else 'disabled'}")
        
        return self.enable_validation