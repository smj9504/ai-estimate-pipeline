# src/models/model_interface_v2.py
"""
ê°œì„ ëœ AI ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ - ì§„í–‰ ìƒí™© ëª¨ë‹ˆí„°ë§ ê¸°ëŠ¥ í¬í•¨
"""
import asyncio
import json
import time
from typing import List, Dict, Any, Optional, Callable
from abc import ABC, abstractmethod
from enum import Enum
from datetime import datetime

import openai
from anthropic import Anthropic
import google.generativeai as genai

from src.models.data_models import ModelResponse
from src.utils.config_loader import ConfigLoader
from src.utils.logger import get_logger, log_model_call, log_error

class ModelStatus(Enum):
    """ëª¨ë¸ í˜¸ì¶œ ìƒíƒœ"""
    IDLE = "idle"
    PREPARING = "preparing"
    SENDING_REQUEST = "sending_request"
    WAITING_RESPONSE = "waiting_response"
    PROCESSING_RESPONSE = "processing_response"
    COMPLETED = "completed"
    ERROR = "error"

class ProgressCallback:
    """ì§„í–‰ ìƒí™© ì½œë°± í´ë˜ìŠ¤"""
    def __init__(self, callback_fn: Optional[Callable] = None):
        self.callback_fn = callback_fn
        self.status_history = []
        
    def update(self, model_name: str, status: ModelStatus, message: str, progress: float = 0.0, details: Dict[str, Any] = None):
        """ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸"""
        update_data = {
            'timestamp': datetime.now().isoformat(),
            'model_name': model_name,
            'status': status.value,
            'message': message,
            'progress': progress,
            'details': details or {}
        }
        
        self.status_history.append(update_data)
        
        if self.callback_fn:
            self.callback_fn(update_data)
        
        # ì½˜ì†” ì¶œë ¥ (ë””ë²„ê¹…ìš©)
        print(f"[{model_name}] {status.value}: {message} ({progress:.0f}%)")

class AIModelInterfaceV2(ABC):
    """ê°œì„ ëœ AI ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ ì¶”ìƒ í´ë˜ìŠ¤"""
    
    def __init__(self, api_key: str, model_name: str):
        self.api_key = api_key
        self.model_name = model_name
        self.max_retries = 3
        self.timeout = 30
        self.progress_callback = ProgressCallback()
        self.current_status = ModelStatus.IDLE
        
    def set_progress_callback(self, callback_fn: Callable):
        """ì§„í–‰ ìƒí™© ì½œë°± í•¨ìˆ˜ ì„¤ì •"""
        self.progress_callback.callback_fn = callback_fn
    
    def _update_progress(self, status: ModelStatus, message: str, progress: float = 0.0, details: Dict[str, Any] = None):
        """ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸ í—¬í¼"""
        self.current_status = status
        self.progress_callback.update(self.model_name, status, message, progress, details)
    
    @abstractmethod
    async def call_model(self, prompt: str, json_data: Dict[str, Any]) -> ModelResponse:
        """ëª¨ë¸ í˜¸ì¶œ ì¶”ìƒ ë©”ì„œë“œ"""
        pass
    
    def _prepare_prompt(self, base_prompt: str, json_data: Dict[str, Any]) -> str:
        """í”„ë¡¬í”„íŠ¸ ì¤€ë¹„"""
        self._update_progress(ModelStatus.PREPARING, "í”„ë¡¬í”„íŠ¸ ì¤€ë¹„ ì¤‘", 10)
        
        json_str = json.dumps(json_data, indent=2, ensure_ascii=False)
        full_prompt = f"{base_prompt}\n\n[JSON DATA]\n{json_str}"
        
        self._update_progress(
            ModelStatus.PREPARING, 
            "í”„ë¡¬í”„íŠ¸ ì¤€ë¹„ ì™„ë£Œ", 
            20,
            {'prompt_size': len(full_prompt), 'json_size': len(json_str)}
        )
        
        return full_prompt
    
    def _extract_response_data(self, raw_response: str) -> Dict[str, Any]:
        """ì‘ë‹µì—ì„œ êµ¬ì¡°í™”ëœ ë°ì´í„° ì¶”ì¶œ"""
        self._update_progress(ModelStatus.PROCESSING_RESPONSE, "ì‘ë‹µ ë°ì´í„° íŒŒì‹± ì¤‘", 80)
        
        try:
            # JSON í˜•íƒœë¡œ ì‘ë‹µì´ ì˜¨ ê²½ìš°
            if raw_response.strip().startswith('{'):
                data = json.loads(raw_response)
                self._update_progress(
                    ModelStatus.PROCESSING_RESPONSE, 
                    "JSON ì‘ë‹µ íŒŒì‹± ì™„ë£Œ", 
                    90,
                    {'response_type': 'json', 'items_count': len(data.get('work_items', []))}
                )
                return data
            
            # í…ìŠ¤íŠ¸ ì‘ë‹µì—ì„œ ì‘ì—… í•­ëª© ì¶”ì¶œ
            lines = raw_response.split('\n')
            work_items = []
            current_room = ""
            
            for line in lines:
                line = line.strip()
                if not line:
                    continue
                
                # ë°© ì´ë¦„ ê°ì§€
                if any(keyword in line.lower() for keyword in ['room', 'kitchen', 'bathroom', 'bedroom']):
                    if ':' in line:
                        current_room = line.split(':')[0].strip()
                
                # ì‘ì—… í•­ëª© ê°ì§€
                if any(line.startswith(prefix) for prefix in ['-', '*', 'â€¢']) or \
                   any(line.startswith(f"{i}.") for i in range(1, 20)):
                    
                    task_name = line.lstrip('-*â€¢0123456789. ').strip()
                    if task_name:
                        work_items.append({
                            'task_name': task_name,
                            'room_name': current_room,
                            'description': task_name,
                            'necessity': 'required'
                        })
            
            self._update_progress(
                ModelStatus.PROCESSING_RESPONSE, 
                "í…ìŠ¤íŠ¸ ì‘ë‹µ íŒŒì‹± ì™„ë£Œ", 
                90,
                {'response_type': 'text', 'items_count': len(work_items)}
            )
            
            return {
                'work_items': work_items,
                'rooms': [{'name': current_room, 'tasks': work_items}] if current_room else []
            }
            
        except Exception as e:
            self._update_progress(
                ModelStatus.ERROR, 
                f"ì‘ë‹µ íŒŒì‹± ì˜¤ë¥˜: {e}", 
                90,
                {'error': str(e)}
            )
            return {
                'work_items': [],
                'rooms': [],
                'raw_text': raw_response
            }

class GPT4InterfaceV2(AIModelInterfaceV2):
    """ê°œì„ ëœ GPT-4 ì¸í„°í˜ì´ìŠ¤"""
    
    def __init__(self, api_key: str):
        super().__init__(api_key, "gpt-4")
        self.client = openai.AsyncOpenAI(api_key=api_key)
        self.logger = get_logger('gpt4_interface_v2')
    
    async def call_model(self, prompt: str, json_data: Dict[str, Any]) -> ModelResponse:
        """GPT-4 ëª¨ë¸ í˜¸ì¶œ with ì§„í–‰ ìƒí™© ëª¨ë‹ˆí„°ë§"""
        start_time = time.time()
        
        try:
            # í”„ë¡¬í”„íŠ¸ ì¤€ë¹„
            full_prompt = self._prepare_prompt(prompt, json_data)
            
            # API í˜¸ì¶œ ì‹œì‘
            self._update_progress(
                ModelStatus.SENDING_REQUEST, 
                "GPT-4 API ìš”ì²­ ì „ì†¡ ì¤‘", 
                30,
                {'model': 'gpt-4', 'max_tokens': 3000}
            )
            
            self.logger.info("GPT-4 API í˜¸ì¶œ ì‹œì‘")
            log_model_call("gpt-4", len(full_prompt))
            
            # ìŠ¤íŠ¸ë¦¬ë° ì˜µì…˜ ì‚¬ìš© (ì§„í–‰ ìƒí™© ì¶”ì  ê°€ëŠ¥)
            stream = await self.client.chat.completions.create(
                model="gpt-4",
                messages=[
                    {
                        "role": "system", 
                        "content": "You are a Senior Reconstruction Estimating Specialist. Analyze the data carefully and provide detailed work scope estimates."
                    },
                    {"role": "user", "content": full_prompt}
                ],
                max_tokens=3000,
                temperature=0.1,
                timeout=self.timeout,
                stream=True  # ìŠ¤íŠ¸ë¦¬ë° í™œì„±í™”
            )
            
            # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìˆ˜ì§‘
            self._update_progress(ModelStatus.WAITING_RESPONSE, "GPT-4 ì‘ë‹µ ìˆ˜ì‹  ì¤‘", 40)
            
            collected_messages = []
            chunk_count = 0
            
            async for chunk in stream:
                if chunk.choices[0].delta.content:
                    collected_messages.append(chunk.choices[0].delta.content)
                    chunk_count += 1
                    
                    # ì§„í–‰ë¥  ê³„ì‚° (40% ~ 70% êµ¬ê°„)
                    progress = 40 + min(30, (chunk_count / 100) * 30)
                    
                    if chunk_count % 10 == 0:  # 10ê°œ ì²­í¬ë§ˆë‹¤ ì—…ë°ì´íŠ¸
                        self._update_progress(
                            ModelStatus.WAITING_RESPONSE, 
                            f"ì‘ë‹µ ìˆ˜ì‹  ì¤‘ (ì²­í¬: {chunk_count})", 
                            progress,
                            {'chunks_received': chunk_count, 'current_size': len(''.join(collected_messages))}
                        )
            
            raw_response = ''.join(collected_messages)
            response_time = time.time() - start_time
            
            self._update_progress(
                ModelStatus.WAITING_RESPONSE, 
                "ì‘ë‹µ ìˆ˜ì‹  ì™„ë£Œ", 
                70,
                {'total_chunks': chunk_count, 'response_size': len(raw_response), 'response_time': response_time}
            )
            
            self.logger.info(f"GPT-4 ì‘ë‹µ ìˆ˜ì‹  (ì†Œìš”ì‹œê°„: {response_time:.2f}ì´ˆ)")
            
            # ì‘ë‹µ ì²˜ë¦¬
            processed_data = self._extract_response_data(raw_response)
            
            # ì™„ë£Œ
            self._update_progress(
                ModelStatus.COMPLETED, 
                "ì²˜ë¦¬ ì™„ë£Œ", 
                100,
                {
                    'total_time': response_time,
                    'work_items': len(processed_data.get('work_items', [])),
                    'rooms': len(processed_data.get('rooms', []))
                }
            )
            
            return ModelResponse(
                model_name=self.model_name,
                room_estimates=processed_data.get('rooms', []),
                processing_time=response_time,
                total_work_items=len(processed_data.get('work_items', [])),
                raw_response=raw_response,
                confidence_self_assessment=0.85
            )
            
        except Exception as e:
            self._update_progress(
                ModelStatus.ERROR, 
                f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}", 
                0,
                {'error': str(e), 'error_type': type(e).__name__}
            )
            
            self.logger.error(f"GPT-4 í˜¸ì¶œ ì˜¤ë¥˜: {e}")
            log_error('gpt4_interface_v2', e, {'prompt_length': len(full_prompt) if 'full_prompt' in locals() else 0})
            
            return ModelResponse(
                model_name=self.model_name,
                room_estimates=[],
                processing_time=time.time() - start_time,
                total_work_items=0,
                raw_response=f"Error: {str(e)}",
                confidence_self_assessment=0.0
            )

class ClaudeInterfaceV2(AIModelInterfaceV2):
    """ê°œì„ ëœ Claude ì¸í„°í˜ì´ìŠ¤"""
    
    def __init__(self, api_key: str):
        super().__init__(api_key, "claude-3-sonnet")
        self.client = Anthropic(api_key=api_key)
        self.logger = get_logger('claude_interface_v2')
    
    async def call_model(self, prompt: str, json_data: Dict[str, Any]) -> ModelResponse:
        """Claude ëª¨ë¸ í˜¸ì¶œ with ì§„í–‰ ìƒí™© ëª¨ë‹ˆí„°ë§"""
        start_time = time.time()
        
        try:
            # í”„ë¡¬í”„íŠ¸ ì¤€ë¹„
            full_prompt = self._prepare_prompt(prompt, json_data)
            
            # API í˜¸ì¶œ ì‹œì‘
            self._update_progress(
                ModelStatus.SENDING_REQUEST, 
                "Claude API ìš”ì²­ ì „ì†¡ ì¤‘", 
                30,
                {'model': 'claude-3-sonnet', 'max_tokens': 3000}
            )
            
            self.logger.info("Claude API í˜¸ì¶œ ì‹œì‘")
            log_model_call("claude-3-sonnet", len(full_prompt))
            
            # ClaudeëŠ” ìŠ¤íŠ¸ë¦¬ë°ì„ ì§€ì›í•˜ì§€ë§Œ ì—¬ê¸°ì„œëŠ” ë‹¨ìˆœ í˜¸ì¶œ ì‚¬ìš©
            self._update_progress(ModelStatus.WAITING_RESPONSE, "Claude ì‘ë‹µ ëŒ€ê¸° ì¤‘", 40)
            
            # asyncio.to_threadë¥¼ ì‚¬ìš©í•œ ë¹„ë™ê¸° ì‹¤í–‰
            response = await asyncio.to_thread(
                self.client.messages.create,
                model="claude-3-sonnet-20240229",
                max_tokens=3000,
                temperature=0.1,
                messages=[{"role": "user", "content": full_prompt}]
            )
            
            raw_response = response.content[0].text
            response_time = time.time() - start_time
            
            self._update_progress(
                ModelStatus.WAITING_RESPONSE, 
                "ì‘ë‹µ ìˆ˜ì‹  ì™„ë£Œ", 
                70,
                {'response_size': len(raw_response), 'response_time': response_time}
            )
            
            self.logger.info(f"Claude ì‘ë‹µ ìˆ˜ì‹  (ì†Œìš”ì‹œê°„: {response_time:.2f}ì´ˆ)")
            
            # ì‘ë‹µ ì²˜ë¦¬
            processed_data = self._extract_response_data(raw_response)
            
            # ì™„ë£Œ
            self._update_progress(
                ModelStatus.COMPLETED, 
                "ì²˜ë¦¬ ì™„ë£Œ", 
                100,
                {
                    'total_time': response_time,
                    'work_items': len(processed_data.get('work_items', [])),
                    'rooms': len(processed_data.get('rooms', []))
                }
            )
            
            return ModelResponse(
                model_name=self.model_name,
                room_estimates=processed_data.get('rooms', []),
                processing_time=response_time,
                total_work_items=len(processed_data.get('work_items', [])),
                raw_response=raw_response,
                confidence_self_assessment=0.88
            )
            
        except Exception as e:
            self._update_progress(
                ModelStatus.ERROR, 
                f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}", 
                0,
                {'error': str(e), 'error_type': type(e).__name__}
            )
            
            self.logger.error(f"Claude í˜¸ì¶œ ì˜¤ë¥˜: {e}")
            log_error('claude_interface_v2', e, {'prompt_length': len(full_prompt) if 'full_prompt' in locals() else 0})
            
            return ModelResponse(
                model_name=self.model_name,
                room_estimates=[],
                processing_time=time.time() - start_time,
                total_work_items=0,
                raw_response=f"Error: {str(e)}",
                confidence_self_assessment=0.0
            )

class GeminiInterfaceV2(AIModelInterfaceV2):
    """ê°œì„ ëœ Gemini ì¸í„°í˜ì´ìŠ¤"""
    
    def __init__(self, api_key: str):
        super().__init__(api_key, "gemini-pro")
        genai.configure(api_key=api_key)
        self.model = genai.GenerativeModel('gemini-pro')
        self.logger = get_logger('gemini_interface_v2')
    
    async def call_model(self, prompt: str, json_data: Dict[str, Any]) -> ModelResponse:
        """Gemini ëª¨ë¸ í˜¸ì¶œ with ì§„í–‰ ìƒí™© ëª¨ë‹ˆí„°ë§"""
        start_time = time.time()
        
        try:
            # í”„ë¡¬í”„íŠ¸ ì¤€ë¹„
            full_prompt = self._prepare_prompt(prompt, json_data)
            
            # API í˜¸ì¶œ ì‹œì‘
            self._update_progress(
                ModelStatus.SENDING_REQUEST, 
                "Gemini API ìš”ì²­ ì „ì†¡ ì¤‘", 
                30,
                {'model': 'gemini-pro', 'max_tokens': 3000}
            )
            
            self.logger.info("Gemini API í˜¸ì¶œ ì‹œì‘")
            log_model_call("gemini-pro", len(full_prompt))
            
            self._update_progress(ModelStatus.WAITING_RESPONSE, "Gemini ì‘ë‹µ ëŒ€ê¸° ì¤‘", 40)
            
            # asyncio.to_threadë¥¼ ì‚¬ìš©í•œ ë¹„ë™ê¸° ì‹¤í–‰
            response = await asyncio.to_thread(
                self.model.generate_content,
                full_prompt,
                generation_config=genai.types.GenerationConfig(
                    max_output_tokens=3000,
                    temperature=0.1
                )
            )
            
            raw_response = response.text
            response_time = time.time() - start_time
            
            self._update_progress(
                ModelStatus.WAITING_RESPONSE, 
                "ì‘ë‹µ ìˆ˜ì‹  ì™„ë£Œ", 
                70,
                {'response_size': len(raw_response), 'response_time': response_time}
            )
            
            self.logger.info(f"Gemini ì‘ë‹µ ìˆ˜ì‹  (ì†Œìš”ì‹œê°„: {response_time:.2f}ì´ˆ)")
            
            # ì‘ë‹µ ì²˜ë¦¬
            processed_data = self._extract_response_data(raw_response)
            
            # ì™„ë£Œ
            self._update_progress(
                ModelStatus.COMPLETED, 
                "ì²˜ë¦¬ ì™„ë£Œ", 
                100,
                {
                    'total_time': response_time,
                    'work_items': len(processed_data.get('work_items', [])),
                    'rooms': len(processed_data.get('rooms', []))
                }
            )
            
            return ModelResponse(
                model_name=self.model_name,
                room_estimates=processed_data.get('rooms', []),
                processing_time=response_time,
                total_work_items=len(processed_data.get('work_items', [])),
                raw_response=raw_response,
                confidence_self_assessment=0.80
            )
            
        except Exception as e:
            self._update_progress(
                ModelStatus.ERROR, 
                f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}", 
                0,
                {'error': str(e), 'error_type': type(e).__name__}
            )
            
            self.logger.error(f"Gemini í˜¸ì¶œ ì˜¤ë¥˜: {e}")
            log_error('gemini_interface_v2', e, {'prompt_length': len(full_prompt) if 'full_prompt' in locals() else 0})
            
            return ModelResponse(
                model_name=self.model_name,
                room_estimates=[],
                processing_time=time.time() - start_time,
                total_work_items=0,
                raw_response=f"Error: {str(e)}",
                confidence_self_assessment=0.0
            )

class ModelOrchestratorV2:
    """ê°œì„ ëœ ëª¨ë¸ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° - ì§„í–‰ ìƒí™© ì¶”ì  ê¸°ëŠ¥ í¬í•¨"""
    
    def __init__(self, config: Optional[ConfigLoader] = None):
        self.config = config or ConfigLoader()
        self.models = {}
        self.logger = get_logger('model_orchestrator_v2')
        self.progress_history = {}
        
    def initialize_models(self, model_names: List[str]) -> Dict[str, bool]:
        """ëª¨ë¸ ì´ˆê¸°í™”"""
        status = {}
        
        for model_name in model_names:
            try:
                if model_name.lower() == 'gpt4':
                    api_key = self.config.get_api_key('openai')
                    if api_key:
                        self.models['gpt4'] = GPT4InterfaceV2(api_key)
                        status['gpt4'] = True
                        self.logger.info("GPT-4 ëª¨ë¸ ì´ˆê¸°í™” ì„±ê³µ")
                    else:
                        status['gpt4'] = False
                        self.logger.warning("OpenAI API í‚¤ ì—†ìŒ")
                        
                elif model_name.lower() == 'claude':
                    api_key = self.config.get_api_key('anthropic')
                    if api_key:
                        self.models['claude'] = ClaudeInterfaceV2(api_key)
                        status['claude'] = True
                        self.logger.info("Claude ëª¨ë¸ ì´ˆê¸°í™” ì„±ê³µ")
                    else:
                        status['claude'] = False
                        self.logger.warning("Anthropic API í‚¤ ì—†ìŒ")
                        
                elif model_name.lower() == 'gemini':
                    api_key = self.config.get_api_key('google')
                    if api_key:
                        self.models['gemini'] = GeminiInterfaceV2(api_key)
                        status['gemini'] = True
                        self.logger.info("Gemini ëª¨ë¸ ì´ˆê¸°í™” ì„±ê³µ")
                    else:
                        status['gemini'] = False
                        self.logger.warning("Google API í‚¤ ì—†ìŒ")
                        
            except Exception as e:
                status[model_name] = False
                self.logger.error(f"{model_name} ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                
        return status
    
    def set_progress_callback(self, model_name: str, callback_fn: Callable):
        """íŠ¹ì • ëª¨ë¸ì˜ ì§„í–‰ ìƒí™© ì½œë°± ì„¤ì •"""
        if model_name in self.models:
            self.models[model_name].set_progress_callback(callback_fn)
    
    def set_global_progress_callback(self, callback_fn: Callable):
        """ëª¨ë“  ëª¨ë¸ì˜ ì§„í–‰ ìƒí™© ì½œë°± ì„¤ì •"""
        for model in self.models.values():
            model.set_progress_callback(callback_fn)
    
    async def call_models_parallel(self, prompt: str, json_data: Dict[str, Any]) -> List[ModelResponse]:
        """ë³‘ë ¬ë¡œ ëª¨ë“  ëª¨ë¸ í˜¸ì¶œ with ì§„í–‰ ìƒí™© ì¶”ì """
        if not self.models:
            self.logger.error("ì´ˆê¸°í™”ëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤")
            return []
        
        self.logger.info(f"ëª¨ë¸ ë³‘ë ¬ ì‹¤í–‰ ì‹œì‘: {list(self.models.keys())}")
        
        # ê° ëª¨ë¸ë³„ ì§„í–‰ ìƒí™© ì´ˆê¸°í™”
        for model_name in self.models.keys():
            self.progress_history[model_name] = []
        
        # ë³‘ë ¬ ì‹¤í–‰
        tasks = []
        for model_name, model_interface in self.models.items():
            task = model_interface.call_model(prompt, json_data)
            tasks.append(task)
        
        # ëª¨ë“  íƒœìŠ¤í¬ ì™„ë£Œ ëŒ€ê¸°
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # ê²°ê³¼ ì²˜ë¦¬
        valid_results = []
        for i, result in enumerate(results):
            model_name = list(self.models.keys())[i]
            
            if isinstance(result, Exception):
                self.logger.error(f"{model_name} ì‹¤í–‰ ì‹¤íŒ¨: {result}")
                # ì‹¤íŒ¨í•œ ê²½ìš° ë¹ˆ ì‘ë‹µ ìƒì„±
                valid_results.append(ModelResponse(
                    model_name=model_name,
                    room_estimates=[],
                    processing_time=0,
                    total_work_items=0,
                    raw_response=f"Error: {str(result)}",
                    confidence_self_assessment=0.0
                ))
            else:
                valid_results.append(result)
                self.logger.info(f"{model_name} ì‹¤í–‰ ì™„ë£Œ: {result.total_work_items} ì‘ì—… í•­ëª©")
        
        return valid_results
    
    def get_progress_summary(self) -> Dict[str, Any]:
        """ì „ì²´ ì§„í–‰ ìƒí™© ìš”ì•½"""
        summary = {
            'models': {},
            'overall_status': 'idle',
            'timestamp': datetime.now().isoformat()
        }
        
        for model_name, model_interface in self.models.items():
            summary['models'][model_name] = {
                'status': model_interface.current_status.value,
                'history_count': len(model_interface.progress_callback.status_history)
            }
            
            # ë§ˆì§€ë§‰ ìƒíƒœ ì •ë³´ ì¶”ê°€
            if model_interface.progress_callback.status_history:
                last_status = model_interface.progress_callback.status_history[-1]
                summary['models'][model_name]['last_update'] = last_status
        
        # ì „ì²´ ìƒíƒœ ê²°ì •
        all_statuses = [m.current_status for m in self.models.values()]
        if any(s == ModelStatus.ERROR for s in all_statuses):
            summary['overall_status'] = 'error'
        elif all(s == ModelStatus.COMPLETED for s in all_statuses):
            summary['overall_status'] = 'completed'
        elif any(s in [ModelStatus.SENDING_REQUEST, ModelStatus.WAITING_RESPONSE, ModelStatus.PROCESSING_RESPONSE] for s in all_statuses):
            summary['overall_status'] = 'processing'
        
        return summary


# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    async def test_with_progress():
        """ì§„í–‰ ìƒí™© ëª¨ë‹ˆí„°ë§ í…ŒìŠ¤íŠ¸"""
        
        # ì§„í–‰ ìƒí™© ì½œë°± í•¨ìˆ˜
        def progress_callback(update_data):
            print(f"ğŸ“Š Progress Update: {update_data}")
        
        # ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° ì´ˆê¸°í™”
        orchestrator = ModelOrchestratorV2()
        
        # ëª¨ë¸ ì´ˆê¸°í™”
        status = orchestrator.initialize_models(['gpt4', 'claude', 'gemini'])
        print(f"Model initialization status: {status}")
        
        # ì „ì—­ ì§„í–‰ ìƒí™© ì½œë°± ì„¤ì •
        orchestrator.set_global_progress_callback(progress_callback)
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„°
        test_prompt = "Analyze the following construction data..."
        test_data = {
            "jobsite": "Test Site",
            "floors": [
                {
                    "location": "1st Floor",
                    "rooms": [
                        {
                            "name": "Living Room",
                            "measurements": {
                                "wall_area_sqft": 300,
                                "ceiling_area_sqft": 150
                            }
                        }
                    ]
                }
            ]
        }
        
        # ëª¨ë¸ í˜¸ì¶œ
        print("\nğŸš€ Starting parallel model execution with progress tracking...\n")
        results = await orchestrator.call_models_parallel(test_prompt, test_data)
        
        # ê²°ê³¼ ì¶œë ¥
        print("\nğŸ“‹ Results:")
        for result in results:
            print(f"  - {result.model_name}: {result.total_work_items} items, {result.processing_time:.2f}s")
        
        # ì§„í–‰ ìƒí™© ìš”ì•½
        summary = orchestrator.get_progress_summary()
        print(f"\nğŸ“Š Progress Summary: {json.dumps(summary, indent=2)}")
    
    # ì‹¤í–‰
    asyncio.run(test_with_progress())