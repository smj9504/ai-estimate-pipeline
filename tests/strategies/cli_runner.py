"""
CLI Runner for Smart Test Framework
ì§€ëŠ¥í˜• í…ŒìŠ¤íŠ¸ í”„ë ˆì„ì›Œí¬ë¥¼ ìœ„í•œ ëª…ë ¹í–‰ ì¸í„°í˜ì´ìŠ¤
"""
import asyncio
import argparse
import json
from typing import List, Dict, Any
from pathlib import Path

from .smart_test_runner import (
    SmartTestRunner, SmartTestConfig, 
    TestDataMode, TestType, TestPriority
)
from ..examples.comprehensive_test_scenarios import ComprehensiveTestScenarios


class SmartTestCLI:
    """ìŠ¤ë§ˆíŠ¸ í…ŒìŠ¤íŠ¸ CLI"""
    
    def __init__(self):
        self.runner = SmartTestRunner()
        self.scenarios = ComprehensiveTestScenarios()
    
    def create_parser(self) -> argparse.ArgumentParser:
        """CLI íŒŒì„œ ìƒì„±"""
        parser = argparse.ArgumentParser(
            description='AI Estimation Pipeline Smart Test Runner',
            formatter_class=argparse.RawDescriptionHelpFormatter,
            epilog="""
Examples:
  # Run CI/CD test suite
  python -m tests.strategies.cli_runner cicd
  
  # Run single phase with specific configuration  
  python -m tests.strategies.cli_runner single --phase 1 --models gpt4 --mode deterministic
  
  # Run performance benchmark
  python -m tests.strategies.cli_runner benchmark --phases 1 2 --runs 5
  
  # Run comprehensive test suite
  python -m tests.strategies.cli_runner comprehensive
  
  # Run custom configuration
  python -m tests.strategies.cli_runner custom --config my_test.json
            """)
        
        subparsers = parser.add_subparsers(dest='command', help='Test commands')
        
        # CI/CD í…ŒìŠ¤íŠ¸
        cicd_parser = subparsers.add_parser('cicd', help='Run CI/CD test suite')
        cicd_parser.add_argument('--fast', action='store_true',
                               help='Run only smoke tests for fastest execution')
        
        # ë‹¨ì¼ í…ŒìŠ¤íŠ¸
        single_parser = subparsers.add_parser('single', help='Run single test')
        single_parser.add_argument('--phase', type=int, required=True,
                                 choices=[0, 1, 2], help='Phase to test')
        single_parser.add_argument('--models', nargs='+',
                                 default=['gpt4'],
                                 choices=['gpt4', 'claude', 'gemini'],
                                 help='AI models to use')
        single_parser.add_argument('--mode', choices=['deterministic', 'live', 'hybrid'],
                                 default='deterministic',
                                 help='Test data mode')
        single_parser.add_argument('--timeout', type=int, default=300,
                                 help='Timeout in seconds')
        single_parser.add_argument('--golden-dataset',
                                 help='Golden dataset name for deterministic mode')
        
        # ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬
        bench_parser = subparsers.add_parser('benchmark', help='Run performance benchmark')
        bench_parser.add_argument('--phases', type=int, nargs='+',
                                default=[1], choices=[0, 1, 2],
                                help='Phases to benchmark')
        bench_parser.add_argument('--runs', type=int, default=3,
                                help='Number of benchmark runs')
        bench_parser.add_argument('--models', nargs='+',
                                default=['gpt4'],
                                choices=['gpt4', 'claude', 'gemini'],
                                help='Models to benchmark')
        
        # ì¢…í•© í…ŒìŠ¤íŠ¸
        comp_parser = subparsers.add_parser('comprehensive', help='Run comprehensive test suite')
        comp_parser.add_argument('--skip-slow', action='store_true',
                               help='Skip slow tests')
        comp_parser.add_argument('--save-results', 
                               help='Save results to specified file')
        
        # ì»¤ìŠ¤í…€ í…ŒìŠ¤íŠ¸
        custom_parser = subparsers.add_parser('custom', help='Run custom test configuration')
        custom_parser.add_argument('--config', required=True,
                                 help='JSON configuration file path')
        
        # ì‹œë‚˜ë¦¬ì˜¤ë³„ í…ŒìŠ¤íŠ¸
        scenario_parser = subparsers.add_parser('scenario', help='Run specific test scenario')
        scenario_parser.add_argument('scenario_name',
                                   choices=['accuracy', 'performance', 'robustness', 'e2e', 'model_comparison'],
                                   help='Scenario to run')
        
        # ê³¨ë“  ë°ì´í„° ê´€ë¦¬
        golden_parser = subparsers.add_parser('golden', help='Golden dataset management')
        golden_subparsers = golden_parser.add_subparsers(dest='golden_command')
        
        # ê³¨ë“  ë°ì´í„° ìƒì„±
        create_parser = golden_subparsers.add_parser('create', help='Create golden dataset')
        create_parser.add_argument('--name', required=True, help='Dataset name')
        create_parser.add_argument('--phases', type=int, nargs='+', required=True,
                                 help='Phases to include')
        create_parser.add_argument('--models', nargs='+', default=['gpt4', 'claude', 'gemini'],
                                 help='Models to use for generation')
        
        # ê³¨ë“  ë°ì´í„° ëª©ë¡
        golden_subparsers.add_parser('list', help='List available golden datasets')
        
        # ì¼ë°˜ ì˜µì…˜
        parser.add_argument('--verbose', '-v', action='store_true',
                          help='Verbose output')
        parser.add_argument('--quiet', '-q', action='store_true',
                          help='Quiet mode (minimal output)')
        parser.add_argument('--output-dir', default='test_outputs/cli_runs',
                          help='Output directory for results')
        
        return parser
    
    async def run_cicd_tests(self, args) -> Dict[str, Any]:
        """CI/CD í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        if not args.quiet:
            print("ğŸš€ Running CI/CD Test Suite...")
        
        if args.fast:
            # ìŠ¤ëª¨í¬ í…ŒìŠ¤íŠ¸ë§Œ
            config = SmartTestConfig(
                name="cicd_smoke",
                phases=[1],
                test_mode=TestDataMode.DETERMINISTIC,
                test_type=TestType.SMOKE,
                priority=TestPriority.CRITICAL,
                models=["gpt4"],
                golden_dataset_name="phase1_baseline",
                timeout_seconds=30,
                max_execution_time=25.0
            )
            return await self.runner.run_smart_test(config)
        else:
            # ì „ì²´ CI/CD ìŠ¤ìœ„íŠ¸
            return await self.scenarios.run_ci_cd_pipeline_test()
    
    async def run_single_test(self, args) -> Dict[str, Any]:
        """ë‹¨ì¼ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        if not args.quiet:
            print(f"ğŸ¯ Running single test: Phase {args.phase}")
        
        # ê³¨ë“  ë°ì´í„°ì…‹ ê²°ì •
        golden_dataset = args.golden_dataset
        if not golden_dataset and args.mode == 'deterministic':
            golden_dataset = f"phase{args.phase}_baseline"
        
        config = SmartTestConfig(
            name=f"single_phase{args.phase}",
            phases=[args.phase],
            test_mode=TestDataMode[args.mode.upper()],
            test_type=TestType.REGRESSION,
            priority=TestPriority.HIGH,
            models=args.models,
            timeout_seconds=args.timeout,
            golden_dataset_name=golden_dataset
        )
        
        return await self.runner.run_smart_test(config)
    
    async def run_benchmark_test(self, args) -> Dict[str, Any]:
        """ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
        if not args.quiet:
            print(f"âš¡ Running performance benchmark: {args.runs} runs")
        
        results = []
        for run in range(args.runs):
            if not args.quiet:
                print(f"  Benchmark run {run + 1}/{args.runs}...")
            
            config = SmartTestConfig(
                name=f"benchmark_run_{run}",
                phases=args.phases,
                test_mode=TestDataMode.DETERMINISTIC,
                test_type=TestType.PERFORMANCE,
                priority=TestPriority.MEDIUM,
                models=args.models,
                golden_dataset_name="phase1_baseline" if 1 in args.phases else "phase0_baseline",
                timeout_seconds=180
            )
            
            result = await self.runner.run_smart_test(config)
            results.append(result)
            
            if not result['success']:
                break
        
        # ë²¤ì¹˜ë§ˆí¬ í†µê³„ ê³„ì‚°
        stats = self._calculate_benchmark_stats(results)
        
        return {
            'benchmark_runs': results,
            'statistics': stats,
            'performance_stable': stats.get('coefficient_of_variation', 1.0) < 0.1
        }
    
    async def run_comprehensive_tests(self, args) -> Dict[str, Any]:
        """ì¢…í•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        if not args.quiet:
            print("ğŸ­ Running Comprehensive Test Suite...")
        
        # TODO: skip_slow ì˜µì…˜ ì²˜ë¦¬
        result = await self.scenarios.run_comprehensive_test_suite()
        
        # ê²°ê³¼ ì €ì¥
        if args.save_results:
            output_file = Path(args.save_results)
            output_file.parent.mkdir(parents=True, exist_ok=True)
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(result, f, indent=2, default=str, ensure_ascii=False)
            
            if not args.quiet:
                print(f"Results saved to: {output_file}")
        
        return result
    
    async def run_custom_test(self, args) -> Dict[str, Any]:
        """ì»¤ìŠ¤í…€ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        config_file = Path(args.config)
        if not config_file.exists():
            raise FileNotFoundError(f"Config file not found: {config_file}")
        
        with open(config_file, 'r', encoding='utf-8') as f:
            config_data = json.load(f)
        
        # JSONì„ SmartTestConfigë¡œ ë³€í™˜
        config = SmartTestConfig(**config_data)
        
        if not args.quiet:
            print(f"ğŸ”§ Running custom test: {config.name}")
        
        return await self.runner.run_smart_test(config)
    
    async def run_scenario_test(self, args) -> Dict[str, Any]:
        """ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        scenario_map = {
            'accuracy': self.scenarios.run_accuracy_validation_test,
            'performance': self.scenarios.run_performance_benchmark_test,
            'robustness': self.scenarios.run_edge_case_robustness_test,
            'e2e': self.scenarios.run_end_to_end_pipeline_test,
            'model_comparison': self.scenarios.run_model_comparison_test
        }
        
        scenario_func = scenario_map[args.scenario_name]
        
        if not args.quiet:
            print(f"ğŸ¬ Running scenario: {args.scenario_name}")
        
        return await scenario_func()
    
    async def manage_golden_data(self, args) -> Dict[str, Any]:
        """ê³¨ë“  ë°ì´í„° ê´€ë¦¬"""
        if args.golden_command == 'create':
            return await self._create_golden_dataset(args)
        elif args.golden_command == 'list':
            return await self._list_golden_datasets(args)
    
    async def _create_golden_dataset(self, args) -> Dict[str, Any]:
        """ê³¨ë“  ë°ì´í„°ì…‹ ìƒì„±"""
        if not args.quiet:
            print(f"ğŸ“¦ Creating golden dataset: {args.name}")
        
        # ì‹¤ì‹œê°„ í…ŒìŠ¤íŠ¸ ì‹¤í–‰í•˜ì—¬ ê²°ê³¼ ìˆ˜ì§‘
        from ..phases.base import PhaseTestConfig
        from ..strategies.test_data_strategy import TestDataManager
        
        test_config = PhaseTestConfig(
            phase_numbers=args.phases,
            models=args.models,
            save_outputs=True
        )
        
        data_manager = TestDataManager()
        live_results = await data_manager.run_live_integration_test(
            args.phases, test_config, save_as_golden=args.name
        )
        
        return {
            'dataset_name': args.name,
            'phases_included': args.phases,
            'models_used': args.models,
            'success': all(r.success for r in live_results)
        }
    
    async def _list_golden_datasets(self, args) -> Dict[str, Any]:
        """ê³¨ë“  ë°ì´í„°ì…‹ ëª©ë¡"""
        from ..strategies.test_data_strategy import TestDataManager
        
        data_manager = TestDataManager()
        datasets = data_manager.list_golden_datasets()
        
        if not args.quiet:
            print("ğŸ“‹ Available Golden Datasets:")
            for dataset in datasets:
                print(f"  â€¢ {dataset}")
        
        return {'datasets': datasets}
    
    def _calculate_benchmark_stats(self, results: List[Dict[str, Any]]) -> Dict[str, float]:
        """ë²¤ì¹˜ë§ˆí¬ í†µê³„ ê³„ì‚°"""
        execution_times = []
        
        for result in results:
            if result.get('success'):
                analysis = result.get('analysis', {})
                perf_metrics = analysis.get('performance_metrics', {})
                if 'avg_execution_time' in perf_metrics:
                    execution_times.append(perf_metrics['avg_execution_time'])
        
        if not execution_times:
            return {}
        
        import statistics
        
        mean_time = statistics.mean(execution_times)
        std_time = statistics.stdev(execution_times) if len(execution_times) > 1 else 0
        
        return {
            'mean_execution_time': mean_time,
            'std_execution_time': std_time,
            'coefficient_of_variation': std_time / mean_time if mean_time > 0 else 0,
            'min_time': min(execution_times),
            'max_time': max(execution_times),
            'runs_count': len(execution_times)
        }
    
    def format_results(self, results: Dict[str, Any], verbose: bool = False) -> str:
        """ê²°ê³¼ í¬ë§¤íŒ…"""
        if not results:
            return "No results to display"
        
        # ê¸°ë³¸ ê²°ê³¼
        success = results.get('success', False)
        status_emoji = "âœ…" if success else "âŒ"
        
        output = [f"{status_emoji} Test Result: {'PASSED' if success else 'FAILED'}"]
        
        # ì‹¤í–‰ ì‹œê°„
        exec_time = results.get('execution_time', 0)
        if exec_time > 0:
            output.append(f"â±ï¸  Execution Time: {exec_time:.2f}s")
        
        # ë¶„ì„ ê²°ê³¼
        analysis = results.get('analysis', {})
        if analysis:
            perf_metrics = analysis.get('performance_metrics', {})
            qual_metrics = analysis.get('quality_metrics', {})
            
            if 'avg_execution_time' in perf_metrics:
                output.append(f"ğŸ“Š Average Execution Time: {perf_metrics['avg_execution_time']:.2f}s")
            
            if 'avg_confidence' in qual_metrics:
                output.append(f"ğŸ¯ Average Confidence: {qual_metrics['avg_confidence']:.1%}")
        
        # Verbose ëª¨ë“œì—ì„œ ì¶”ê°€ ì •ë³´
        if verbose:
            if 'config' in results:
                config = results['config']
                output.append(f"ğŸ”§ Configuration: {config.get('name', 'N/A')}")
                output.append(f"   Models: {config.get('models', [])}")
                output.append(f"   Phases: {config.get('phases', [])}")
            
            # ê¶Œì¥ì‚¬í•­
            recommendations = analysis.get('recommendations', [])
            if recommendations:
                output.append("ğŸ’¡ Recommendations:")
                for rec in recommendations:
                    output.append(f"   â€¢ {rec}")
        
        return "\n".join(output)
    
    async def run(self):
        """CLI ì‹¤í–‰"""
        parser = self.create_parser()
        args = parser.parse_args()
        
        if not args.command:
            parser.print_help()
            return
        
        try:
            # ëª…ë ¹ ì‹¤í–‰
            if args.command == 'cicd':
                results = await self.run_cicd_tests(args)
            elif args.command == 'single':
                results = await self.run_single_test(args)
            elif args.command == 'benchmark':
                results = await self.run_benchmark_test(args)
            elif args.command == 'comprehensive':
                results = await self.run_comprehensive_tests(args)
            elif args.command == 'custom':
                results = await self.run_custom_test(args)
            elif args.command == 'scenario':
                results = await self.run_scenario_test(args)
            elif args.command == 'golden':
                results = await self.manage_golden_data(args)
            else:
                parser.print_help()
                return
            
            # ê²°ê³¼ ì¶œë ¥
            if not args.quiet:
                print("\n" + "="*50)
                print(self.format_results(results, args.verbose))
                print("="*50)
            
            # JSON ì¶œë ¥ ì˜µì…˜
            if hasattr(args, 'json_output') and args.json_output:
                print(json.dumps(results, indent=2, default=str))
                
        except Exception as e:
            if args.verbose:
                import traceback
                print(f"âŒ Error: {e}")
                print(traceback.format_exc())
            else:
                print(f"âŒ Error: {e}")


def main():
    """ë©”ì¸ ì§„ì…ì """
    cli = SmartTestCLI()
    asyncio.run(cli.run())


if __name__ == "__main__":
    main()