"""
Comprehensive Test Scenarios - AI Estimation Pipeline
í¬ê´„ì ì¸ í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤ ì˜ˆì œ ëª¨ìŒ
"""
import asyncio
from datetime import datetime
from pathlib import Path

# Import test framework components
from ..strategies.smart_test_runner import SmartTestRunner, SmartTestConfig
from ..strategies.test_data_strategy import TestDataManager, TestDataMode
from ..strategies.smart_test_runner import TestType, TestPriority


class ComprehensiveTestScenarios:
    """í¬ê´„ì ì¸ í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤ ëª¨ìŒ"""
    
    def __init__(self):
        self.smart_runner = SmartTestRunner("test_outputs/comprehensive_scenarios")
        self.data_manager = TestDataManager()
    
    # ============ Scenario 1: CI/CD íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ============
    
    async def run_ci_cd_pipeline_test(self) -> Dict[str, Any]:
        """
        CI/CD íŒŒì´í”„ë¼ì¸ìš© ë¹ ë¥¸ íšŒê·€ í…ŒìŠ¤íŠ¸
        - ê²°ì •ì  ë°ì´í„°ë¡œ ë¹ ë¥¸ ì‹¤í–‰
        - Critical ìš°ì„ ìˆœìœ„ë¡œ ì‹¤íŒ¨ì‹œ ì¦‰ì‹œ ì¤‘ë‹¨
        - ë‹¨ì¼ ëª¨ë¸ë¡œ ì„±ëŠ¥ ìµœì í™”
        """
        print("ğŸš€ Running CI/CD Pipeline Test...")
        
        # 1ë‹¨ê³„: ìŠ¤ëª¨í¬ í…ŒìŠ¤íŠ¸ (30ì´ˆ ì´ë‚´)
        smoke_config = SmartTestConfig(
            name="cicd_smoke_test",
            phases=[1],  # Phase 1ë§Œ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸
            test_mode=TestDataMode.DETERMINISTIC,
            test_type=TestType.SMOKE,
            priority=TestPriority.CRITICAL,
            models=["gpt4"],  # ê°€ì¥ ì•ˆì •ì ì¸ ëª¨ë¸ë§Œ
            golden_dataset_name="phase1_baseline",
            timeout_seconds=30,
            max_execution_time=25.0,
            min_confidence_score=0.6,  # ë‚®ì€ ì„ê³„ê°’ìœ¼ë¡œ ë¹ ë¥¸ íŒ¨ìŠ¤
            failure_tolerance=0.0  # ì‹¤íŒ¨ í—ˆìš© ì—†ìŒ
        )
        
        # 2ë‹¨ê³„: í†µí•© í…ŒìŠ¤íŠ¸ (2ë¶„ ì´ë‚´)
        integration_config = SmartTestConfig(
            name="cicd_integration_test",
            phases=[0, 1, 2],
            test_mode=TestDataMode.DETERMINISTIC,
            test_type=TestType.INTEGRATION,
            priority=TestPriority.HIGH,
            models=["gpt4"],
            golden_dataset_name="pipeline_baseline",
            timeout_seconds=120,
            max_execution_time=100.0,
            min_confidence_score=0.7,
            parallel_execution=True  # Phaseë³„ ë³‘ë ¬ ì‹¤í–‰
        )
        
        # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        suite_result = await self.smart_runner.run_test_suite([
            smoke_config, integration_config
        ])
        
        return suite_result
    
    # ============ Scenario 2: ì •í™•ë„ ê²€ì¦ í…ŒìŠ¤íŠ¸ ============
    
    async def run_accuracy_validation_test(self) -> Dict[str, Any]:
        """
        AI ëª¨ë¸ ì •í™•ë„ ê²€ì¦ í…ŒìŠ¤íŠ¸
        - ì‹¤ì‹œê°„ ë°ì´í„°ë¡œ ì‹¤ì œ ì„±ëŠ¥ ì¸¡ì •
        - ë©€í‹° ëª¨ë¸ í•©ì˜ ê²€ì¦
        - ê³¨ë“  ë°ì´í„°ì…‹ ìë™ ìƒì„±
        """
        print("ğŸ¯ Running Accuracy Validation Test...")
        
        # ì‹¤ì‹œê°„ ì •í™•ë„ í…ŒìŠ¤íŠ¸
        accuracy_config = SmartTestConfig(
            name="accuracy_validation",
            phases=[1, 2],
            test_mode=TestDataMode.LIVE,
            test_type=TestType.REGRESSION,
            priority=TestPriority.HIGH,
            models=["gpt4", "claude", "gemini"],  # 3ê°œ ëª¨ë¸ í•©ì˜
            timeout_seconds=300,
            max_execution_time=200.0,
            min_confidence_score=0.8,  # ë†’ì€ ì‹ ë¢°ë„ ìš”êµ¬
            min_consensus_level=0.7,   # ëª¨ë¸ê°„ í•©ì˜ë„ ìš”êµ¬
            save_as_golden="accuracy_validation_baseline",  # ê²°ê³¼ë¥¼ ê³¨ë“ ìœ¼ë¡œ ì €ì¥
            retry_attempts=3
        )
        
        result = await self.smart_runner.run_smart_test(accuracy_config)
        
        # ì •í™•ë„ ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±
        if result['success']:
            await self._generate_accuracy_report(result)
        
        return result
    
    # ============ Scenario 3: ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸ ============
    
    async def run_performance_benchmark_test(self) -> Dict[str, Any]:
        """
        ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ë° íšŒê·€ ê°ì§€ í…ŒìŠ¤íŠ¸
        - ë°˜ë³µ ì‹¤í–‰ìœ¼ë¡œ ì•ˆì •ì„± ì¸¡ì •
        - ì„±ëŠ¥ íŠ¸ë Œë“œ ë¶„ì„
        - ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§
        """
        print("âš¡ Running Performance Benchmark Test...")
        
        performance_config = SmartTestConfig(
            name="performance_benchmark",
            phases=[1],  # Phase 1 ì§‘ì¤‘ ë²¤ì¹˜ë§ˆí¬
            test_mode=TestDataMode.DETERMINISTIC,
            test_type=TestType.PERFORMANCE,
            priority=TestPriority.MEDIUM,
            models=["gpt4"],
            golden_dataset_name="phase1_baseline",
            timeout_seconds=180,
            max_execution_time=45.0,  # 45ì´ˆ ì„±ëŠ¥ ëª©í‘œ
            min_confidence_score=0.75,
            performance_baseline={
                'avg_execution_time': 40.0,
                'avg_confidence': 0.8,
                'model_success_rate': 0.95
            }
        )
        
        # ì—¬ëŸ¬ ë²ˆ ì‹¤í–‰í•˜ì—¬ ì„±ëŠ¥ ì•ˆì •ì„± ì¸¡ì •
        benchmark_results = []
        for run in range(5):  # 5íšŒ ë°˜ë³µ
            print(f"  Performance run {run + 1}/5...")
            result = await self.smart_runner.run_smart_test(performance_config)
            benchmark_results.append(result)
            
            # ì‹¤íŒ¨ì‹œ ì¤‘ë‹¨
            if not result['success']:
                break
        
        # ì„±ëŠ¥ í†µê³„ ê³„ì‚°
        performance_stats = self._calculate_performance_stats(benchmark_results)
        
        return {
            'benchmark_results': benchmark_results,
            'performance_statistics': performance_stats,
            'performance_regression_detected': performance_stats['avg_time'] > performance_config.max_execution_time
        }
    
    # ============ Scenario 4: ì—£ì§€ ì¼€ì´ìŠ¤ ê²¬ê³ ì„± í…ŒìŠ¤íŠ¸ ============
    
    async def run_edge_case_robustness_test(self) -> Dict[str, Any]:
        """
        ì—£ì§€ ì¼€ì´ìŠ¤ ë° ì˜¤ë¥˜ ì²˜ë¦¬ ê²¬ê³ ì„± í…ŒìŠ¤íŠ¸
        - ê·¹ë‹¨ì ì¸ ì…ë ¥ ë°ì´í„°
        - ì˜¤ë¥˜ ì¡°ê±´ ì‹œë®¬ë ˆì´ì…˜
        - ë³µêµ¬ ëŠ¥ë ¥ ê²€ì¦
        """
        print("ğŸ›¡ï¸ Running Edge Case Robustness Test...")
        
        edge_cases = [
            # ë¹ˆ ë°ì´í„° ì¼€ì´ìŠ¤
            {
                'name': 'empty_data_handling',
                'description': 'ë¹ˆ ë°ì´í„° ì²˜ë¦¬ ëŠ¥ë ¥',
                'data_override': {'data': []}
            },
            # ê·¹ë‹¨ì  ìˆ˜ì¹˜ ì¼€ì´ìŠ¤
            {
                'name': 'extreme_values_handling',
                'description': 'ê·¹ë‹¨ì  ìˆ˜ì¹˜ ì²˜ë¦¬',
                'data_override': {
                    'measurements': {'width': 1000000, 'length': 0.001}
                }
            },
            # ë¶ˆì™„ì „í•œ ë°ì´í„° ì¼€ì´ìŠ¤
            {
                'name': 'incomplete_data_handling',
                'description': 'ë¶ˆì™„ì „í•œ ë°ì´í„° ì²˜ë¦¬',
                'data_override': {
                    'rooms': [{'name': 'Test', 'work_scope': {}}]  # measurements ëˆ„ë½
                }
            }
        ]
        
        edge_case_results = []
        
        for case in edge_cases:
            print(f"  Testing: {case['description']}...")
            
            config = SmartTestConfig(
                name=case['name'],
                phases=[1],
                test_mode=TestDataMode.DETERMINISTIC,
                test_type=TestType.REGRESSION,
                priority=TestPriority.MEDIUM,
                models=["gpt4"],
                golden_dataset_name="phase1_baseline",
                timeout_seconds=60,
                failure_tolerance=1.0,  # ì‹¤íŒ¨ í—ˆìš© (ê²¬ê³ ì„± í…ŒìŠ¤íŠ¸)
                retry_attempts=1  # ì¬ì‹œë„ ìµœì†Œí™”
            )
            
            # TODO: ì‹¤ì œë¡œëŠ” data_overrideë¥¼ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ì ìš©í•˜ëŠ” ë¡œì§ í•„ìš”
            result = await self.smart_runner.run_smart_test(config)
            result['edge_case'] = case
            edge_case_results.append(result)
        
        # ê²¬ê³ ì„± ì ìˆ˜ ê³„ì‚°
        robustness_score = sum(1 for r in edge_case_results if r['success']) / len(edge_cases)
        
        return {
            'edge_case_results': edge_case_results,
            'robustness_score': robustness_score,
            'robust_system': robustness_score >= 0.7  # 70% ì´ìƒ í†µê³¼ì‹œ ê²¬ê³ í•¨
        }
    
    # ============ Scenario 5: ì „ì²´ íŒŒì´í”„ë¼ì¸ ì¢…ë‹¨ê°„ í…ŒìŠ¤íŠ¸ ============
    
    async def run_end_to_end_pipeline_test(self) -> Dict[str, Any]:
        """
        ì „ì²´ íŒŒì´í”„ë¼ì¸ ì¢…ë‹¨ê°„ í…ŒìŠ¤íŠ¸
        - Phase 0-2 ì „ì²´ ì‹¤í–‰
        - ë°ì´í„° íë¦„ ë¬´ê²°ì„± ê²€ì¦
        - ì‹¤ì œ ì›Œí¬í”Œë¡œ ì‹œë®¬ë ˆì´ì…˜
        """
        print("ğŸ”„ Running End-to-End Pipeline Test...")
        
        e2e_config = SmartTestConfig(
            name="end_to_end_pipeline",
            phases=[0, 1, 2],
            test_mode=TestDataMode.LIVE,  # ì‹¤ì œ AI í˜¸ì¶œ
            test_type=TestType.INTEGRATION,
            priority=TestPriority.HIGH,
            models=["gpt4", "claude"],  # 2ê°œ ëª¨ë¸ë¡œ ê· í˜•
            timeout_seconds=600,  # 10ë¶„ ì œí•œ
            max_execution_time=400.0,
            min_confidence_score=0.75,
            min_consensus_level=0.6,
            save_as_golden="e2e_pipeline_baseline",
            parallel_execution=False  # ìˆœì°¨ ì‹¤í–‰ìœ¼ë¡œ ë°ì´í„° íë¦„ ë³´ì¥
        )
        
        result = await self.smart_runner.run_smart_test(e2e_config)
        
        # íŒŒì´í”„ë¼ì¸ í’ˆì§ˆ ë¶„ì„
        pipeline_quality = self._analyze_pipeline_quality(result)
        
        return {
            'e2e_result': result,
            'pipeline_quality': pipeline_quality,
            'data_flow_integrity': pipeline_quality['data_consistency_score'] > 0.8
        }
    
    # ============ Scenario 6: ëª¨ë¸ ë¹„êµ í…ŒìŠ¤íŠ¸ ============
    
    async def run_model_comparison_test(self) -> Dict[str, Any]:
        """
        AI ëª¨ë¸ë³„ ì„±ëŠ¥ ë¹„êµ í…ŒìŠ¤íŠ¸
        - ê°œë³„ ëª¨ë¸ ì„±ëŠ¥ ì¸¡ì •
        - ëª¨ë¸ê°„ ì¼ê´€ì„± ë¶„ì„
        - ìµœì  ëª¨ë¸ ì¡°í•© ì œì•ˆ
        """
        print("ğŸ“Š Running Model Comparison Test...")
        
        models_to_test = ["gpt4", "claude", "gemini"]
        model_results = {}
        
        # ê° ëª¨ë¸ ê°œë³„ í…ŒìŠ¤íŠ¸
        for model in models_to_test:
            print(f"  Testing model: {model}...")
            
            config = SmartTestConfig(
                name=f"model_comparison_{model}",
                phases=[1, 2],
                test_mode=TestDataMode.DETERMINISTIC,
                test_type=TestType.PERFORMANCE,
                priority=TestPriority.MEDIUM,
                models=[model],  # ë‹¨ì¼ ëª¨ë¸
                golden_dataset_name="phase1_baseline",
                timeout_seconds=200,
                max_execution_time=120.0
            )
            
            result = await self.smart_runner.run_smart_test(config)
            model_results[model] = result
        
        # ëª¨ë¸ë³„ ë¹„êµ ë¶„ì„
        comparison_analysis = self._compare_model_performance(model_results)
        
        return {
            'individual_results': model_results,
            'comparison_analysis': comparison_analysis,
            'recommended_models': comparison_analysis['top_performers']
        }
    
    # ============ ì¢…í•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ============
    
    async def run_comprehensive_test_suite(self) -> Dict[str, Any]:
        """ì „ì²´ í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤ ì‹¤í–‰"""
        print("ğŸ­ Starting Comprehensive Test Suite...")
        start_time = datetime.now()
        
        results = {}
        
        try:
            # 1. CI/CD í…ŒìŠ¤íŠ¸ (í•„ìˆ˜)
            results['cicd_pipeline'] = await self.run_ci_cd_pipeline_test()
            
            # CI/CD ì‹¤íŒ¨ì‹œ ì¤‘ë‹¨
            if not results['cicd_pipeline']['overall_success']:
                print("âŒ CI/CD tests failed - stopping comprehensive suite")
                return results
            
            # 2. ì •í™•ë„ ê²€ì¦
            results['accuracy_validation'] = await self.run_accuracy_validation_test()
            
            # 3. ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬
            results['performance_benchmark'] = await self.run_performance_benchmark_test()
            
            # 4. ê²¬ê³ ì„± í…ŒìŠ¤íŠ¸
            results['edge_case_robustness'] = await self.run_edge_case_robustness_test()
            
            # 5. ì¢…ë‹¨ê°„ í…ŒìŠ¤íŠ¸
            results['end_to_end_pipeline'] = await self.run_end_to_end_pipeline_test()
            
            # 6. ëª¨ë¸ ë¹„êµ
            results['model_comparison'] = await self.run_model_comparison_test()
            
        except Exception as e:
            results['error'] = str(e)
        
        execution_time = (datetime.now() - start_time).total_seconds()
        
        # ì¢…í•© ë¶„ì„
        overall_analysis = self._generate_comprehensive_analysis(results)
        
        return {
            'execution_time': execution_time,
            'test_results': results,
            'overall_analysis': overall_analysis,
            'comprehensive_success': overall_analysis['overall_health_score'] > 0.8
        }
    
    # ============ ë¶„ì„ ìœ í‹¸ë¦¬í‹° ë©”ì„œë“œ ============
    
    async def _generate_accuracy_report(self, result: Dict[str, Any]):
        """ì •í™•ë„ ë¦¬í¬íŠ¸ ìƒì„±"""
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ìƒì„¸í•œ ì •í™•ë„ ë¶„ì„ ë¡œì§ ì¶”ê°€
        pass
    
    def _calculate_performance_stats(self, results: List[Dict[str, Any]]) -> Dict[str, float]:
        """ì„±ëŠ¥ í†µê³„ ê³„ì‚°"""
        execution_times = []
        confidence_scores = []
        
        for result in results:
            if result.get('success'):
                analysis = result.get('analysis', {})
                perf_metrics = analysis.get('performance_metrics', {})
                qual_metrics = analysis.get('quality_metrics', {})
                
                if 'avg_execution_time' in perf_metrics:
                    execution_times.append(perf_metrics['avg_execution_time'])
                if 'avg_confidence' in qual_metrics:
                    confidence_scores.append(qual_metrics['avg_confidence'])
        
        if not execution_times:
            return {'avg_time': 0, 'std_time': 0, 'avg_confidence': 0}
        
        import statistics
        
        return {
            'avg_time': statistics.mean(execution_times),
            'std_time': statistics.stdev(execution_times) if len(execution_times) > 1 else 0,
            'avg_confidence': statistics.mean(confidence_scores) if confidence_scores else 0,
            'stability_score': 1.0 - (statistics.stdev(execution_times) / statistics.mean(execution_times)) if len(execution_times) > 1 else 1.0
        }
    
    def _analyze_pipeline_quality(self, result: Dict[str, Any]) -> Dict[str, float]:
        """íŒŒì´í”„ë¼ì¸ í’ˆì§ˆ ë¶„ì„"""
        return {
            'data_consistency_score': 0.9,  # TODO: ì‹¤ì œ ë°ì´í„° ì¼ê´€ì„± ê²€ì‚¬
            'phase_transition_score': 0.85,  # TODO: Phaseê°„ ì „í™˜ í’ˆì§ˆ
            'overall_quality_score': 0.87
        }
    
    def _compare_model_performance(self, model_results: Dict[str, Dict[str, Any]]) -> Dict[str, Any]:
        """ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ë¶„ì„"""
        performance_scores = {}
        
        for model, result in model_results.items():
            if result.get('success'):
                analysis = result.get('analysis', {})
                perf_metrics = analysis.get('performance_metrics', {})
                qual_metrics = analysis.get('quality_metrics', {})
                
                # ì¢…í•© ì ìˆ˜ ê³„ì‚° (ì‹¤í–‰ì‹œê°„ ì—­ìˆ˜ + ì‹ ë¢°ë„)
                exec_time = perf_metrics.get('avg_execution_time', 999)
                confidence = qual_metrics.get('avg_confidence', 0)
                
                performance_scores[model] = (1.0 / exec_time) * 100 + confidence
        
        # ìƒìœ„ ëª¨ë¸ ì„ ë³„
        sorted_models = sorted(performance_scores.items(), key=lambda x: x[1], reverse=True)
        
        return {
            'performance_scores': performance_scores,
            'top_performers': [model for model, score in sorted_models[:2]],
            'best_model': sorted_models[0][0] if sorted_models else None
        }
    
    def _generate_comprehensive_analysis(self, results: Dict[str, Any]) -> Dict[str, Any]:
        """ì¢…í•© ë¶„ì„ ìƒì„±"""
        success_count = 0
        total_tests = 0
        
        for test_name, result in results.items():
            if test_name != 'error':
                total_tests += 1
                if isinstance(result, dict):
                    if result.get('overall_success') or result.get('success') or result.get('comprehensive_success'):
                        success_count += 1
        
        health_score = success_count / total_tests if total_tests > 0 else 0
        
        return {
            'total_tests_run': total_tests,
            'tests_passed': success_count,
            'overall_health_score': health_score,
            'system_status': 'HEALTHY' if health_score > 0.8 else 'NEEDS_ATTENTION' if health_score > 0.6 else 'CRITICAL',
            'recommendations': self._generate_recommendations(health_score, results)
        }
    
    def _generate_recommendations(self, health_score: float, results: Dict[str, Any]) -> List[str]:
        """ê°œì„  ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        if health_score < 0.8:
            recommendations.append("System health below optimal - review failed test cases")
        
        if 'performance_benchmark' in results:
            perf_result = results['performance_benchmark']
            if perf_result.get('performance_regression_detected'):
                recommendations.append("Performance regression detected - investigate recent changes")
        
        if 'edge_case_robustness' in results:
            robustness = results['edge_case_robustness']
            if not robustness.get('robust_system', True):
                recommendations.append("System robustness needs improvement - enhance error handling")
        
        return recommendations


# ============ ì‹¤í–‰ ì˜ˆì œ ============

async def main():
    """ì¢…í•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì˜ˆì œ"""
    scenarios = ComprehensiveTestScenarios()
    
    # ê°œë³„ ì‹œë‚˜ë¦¬ì˜¤ ì‹¤í–‰ ì˜ˆì œ
    print("Running individual test scenarios...\n")
    
    # CI/CD í…ŒìŠ¤íŠ¸
    cicd_result = await scenarios.run_ci_cd_pipeline_test()
    print(f"CI/CD Test: {'âœ… PASSED' if cicd_result['overall_success'] else 'âŒ FAILED'}")
    
    # ì „ì²´ í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸ ì‹¤í–‰
    print("\nRunning comprehensive test suite...\n")
    
    comprehensive_result = await scenarios.run_comprehensive_test_suite()
    
    # ê²°ê³¼ ìš”ì•½
    analysis = comprehensive_result['overall_analysis']
    print(f"\nğŸ¯ Test Suite Results:")
    print(f"   Tests Run: {analysis['total_tests_run']}")
    print(f"   Tests Passed: {analysis['tests_passed']}")
    print(f"   Health Score: {analysis['overall_health_score']:.1%}")
    print(f"   System Status: {analysis['system_status']}")
    print(f"   Overall Success: {'âœ… PASSED' if comprehensive_result['comprehensive_success'] else 'âŒ FAILED'}")
    
    if analysis['recommendations']:
        print(f"\nğŸ“‹ Recommendations:")
        for rec in analysis['recommendations']:
            print(f"   â€¢ {rec}")


if __name__ == "__main__":
    asyncio.run(main())